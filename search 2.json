[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ekarin Eric Pongpipat",
    "section": "",
    "text": "I am interested in understanding the relationship between brain architecture (e.g., networks and gradients) and cognition, and how this relationship can change as we age and for different groups of individuals (e.g., disease-disorder states and genetic predispositions). I’m also interested in statistics, research methods, intuitive data visualization, and open science practices (open source code, open data, and open publication access)."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am primarly interested in understanding the relationship between brain networks and cognition and how this relationship can change as we age and for different groups of individuals (e.g., disease-disorder states and genetic predispositions). I’m also interested in statistics, research methods, intuitive data visualization, open source, and open science."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent\n\n\n\n\n\n\ngeneral linear model (GLM)\n\n\ngradient descent\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJun 10, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nExploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis\n\n\n\n\n\n\nprincipal components analysis (PCA)\n\n\nR\n\n\nstatistics\n\n\n\nA collaboraion with Matt Kmiecik using principal components analysis to analyze Blackhawk’s data.\n\n\n\n\n\nMar 7, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nSolving Ordinary Least Squares (OLS) Regression Using Matrix Algebra\n\n\n\n\n\n\nmatrix algebra\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJan 30, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Linear Regression via Voice Command: A Shiny App\n\n\n\n\n\n\ngeneral linear model (GLM)\n\n\nshiny app\n\n\nstatistics\n\n\n\nChange variables of a simple linear regression via voice commands. (Note: Link may or not work depending on if my free hours have been used up for the month, but feel free to download the code and run locally from my GitHub.)\n\n\n\n\n\nJun 17, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrapping and Permutation Testing: A Shiny App\n\n\n\n\n\n\nbootstrapping\n\n\npermutation\n\n\nR\n\n\nshiny app\n\n\nstatistics\n\n\n\nA collaboration post with Matt Kmiecik about the differences between bootstrapping and permutation testing with input variables that change on the fly.\n\n\n\n\n\nJun 7, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate 24-Motion Variables\n\n\nA quick guide to calculating 24-motion variables\n\n\n\nfMRI\n\n\nmotion\n\n\nR\n\n\npreprocessing\n\n\n\n\n\n\n\n\n\nMar 20, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nMultilevel Modeling in R with NHL Power Play Data\n\n\n\n\n\n\nmultilevel modeling (MLM)\n\n\nR\n\n\nstatistics\n\n\n\nA collaboration post with Matt Kmiecik about multilevel modeling in R using professional hockey data from the NHL.\n\n\n\n\n\nFeb 11, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 7, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 4, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Git & GitHub Tutorial for Scientists: It’s Not Only for Programmers (2019)By Micaela Chan, Ekarin Pongpipat, & Luke MoragliaA collaborative ebook that aims to provide a practical tutorial on using git and GitHub for scientists.[url] [github]\nA Practical Extension of Introductory Statistics in Psychology using R (2019)By Ekarin E. Pongpipat, Giuseppe G. Miranda, & Matthew J. KmiecikA collaborative ebook that aims to provide a practical extension of introductory statistics typically taught in psychology into the general linear model (GLM) using R.[url] [github]"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "1 Education\n\n\n2 Research\n\n\n3 Teaching Experience\n\n\n4 Publications\n\n\n5 Poster Presentations\n\n\n6 Talks and Workshops\n\n\n7 Honors and Awards\n\n\n8 Professional Society Memberships"
  },
  {
    "objectID": "cv.html#research",
    "href": "cv.html#research",
    "title": "Curriculum Vitae",
    "section": "Research",
    "text": "Research\n\n\n\n\n\n\n\n\n\nGraduate Research Assistant (2017 - 2025) Cognitive Neuroscience of Aging Laboratory Center for Vital Longevity School of Behavioral and Brain Sciences The University of Texas at Dallas \n\n\n\n\n\n\n\nGraduate Research Assistant (2014 - 2016) Lifespan Human Senses Laboratory Department of Psychology San Diego State University \n\n\n\n\n\n\n\nUndergraduate Research Assistant (2011 - 2013) Neuroscience Laboratory Department of Psychology California State University, Northridge"
  },
  {
    "objectID": "cv.html#teaching-experience",
    "href": "cv.html#teaching-experience",
    "title": "Curriculum Vitae",
    "section": "Teaching Experience",
    "text": "Teaching Experience\n\n\n\n\n\n\n\n\n\nGraduate Teaching Assistant (2017 - 2019) The University of Texas at Dallas     1. Statistics for Psychology, ACN6312/HCS6312 (Spring 2019)         Under the supervision of Dr. Nancy Juhn\n\n    2. Statistics for Psychology, PSY2317 (Fall 2017, Spring 2018, Fall 2018)         Under the supervision of Dr. Nancy Juhn \n\n\n\n\n\n\n\nGraduate Teaching Assistant (2014 - 2016) San Diego State University     1. Advanced Statistics in Psychology, PSY670A/B (Fall 2015, Spring 2016)         Under the supervision of Dr. Melody Sadler\n\n    2. Statistical Methods in Psychology Laboratory, PSY280L (Fall 2014, Spring 2015)         Under the supervision of Dr. Melody Sadler, Dr. Amy Spilkin, and Dr. Dale Glaser"
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "Curriculum Vitae",
    "section": "Publications",
    "text": "Publications\n\nPongpipat, E. E., Kennedy, K. M., Foster, C. M., Boylan, M. A., & Rodrigue, K. M (2021). Functional connectivity within and between n-back modulated regions: An adult lifespan PPI investigation. Brain Connectivity, 11(2), . https://doi.org/10.1089/brain.2020.0791.          \n\n\n\nBoylan, M. A., Foster, C. M., Pongpipat, E. E., Webb, C. E., Rodrigue, K. M. & Kennedy K. K. (2021). Greater BOLD variability is associated with poorer cognitive function in an adult lifespan sample. Cerebral Cortex, 31(1), 562-574. https://doi.org/10.1093/cercor/bhaa243.          \n\n\n\nMcIntosh, E. C., Jacobson, A., Kemmotsu, N., Pongpipat, E. E., Green, E., Haase, L., & Murphy, C. (2017). Does medial temporal lobe thickness mediate the association between risk factor burden and memory performance in middle-aged or older adults with metabolic syndrome?. Neuroscience Letters, 636, 225-232. https://doi.org/10.1016/j.neulet.2016.10.010."
  },
  {
    "objectID": "cv.html#poster-presentations",
    "href": "cv.html#poster-presentations",
    "title": "Curriculum Vitae",
    "section": "Poster Presentations",
    "text": "Poster Presentations"
  },
  {
    "objectID": "cv.html#talks-and-workshops",
    "href": "cv.html#talks-and-workshops",
    "title": "Curriculum Vitae",
    "section": "Talks and Workshops",
    "text": "Talks and Workshops\n\n\n\n\n\n\n2022-10\n\n\nMulti-Level Modeling Lab WorkshopCognitive Neuroscience of Aging LaboratoryCenter for Vital LongevityBehavioral and Brain SciencesThe University of Texas at Dallas[url] [github]\n\n\n\n\n2019-11\n\n\nBrianhack Dallas 2019Behavioral and Brain SciencesThe University of Texas at Dallas[url] [github]\n\n\n\n\n2019-10\n\n\nTalk on ‘Functional connectivity of the fronto-parietal and default mode networks during the n-back across the adult lifespan’Developmental, Cognitive and Social/Personality Brownbag SeriesBehavioral and Brain SciencesThe University of Texas at Dallas\n\n\n\n\n2019-09\n\n\nAssisted in an introduction to Git/GitHub Workshop with Dr. Micaela ChanCenter for Vital LongevityBehavioral and Brain SciencesThe University of Texas at Dallas[url] [github]\n\n\n\n\n2019-04\n\n\nGuest lectured on using R for descriptive and inferential statistics Dr. Nancy Juhn’s Research Methods 2 Course (ACN6313/HCS6313) Behavioral and Brain SciencesThe University of Texas at Dallas"
  },
  {
    "objectID": "cv.html#honors-and-awards",
    "href": "cv.html#honors-and-awards",
    "title": "Curriculum Vitae",
    "section": "Honors and Awards",
    "text": "Honors and Awards\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHonor\nInstitution\nAward\n\n\n\n\n2023\nStudent Scholarship Award\nThe University of Texas at Dallas\n$1,000\n\n\n2020\nStudent Travel Award\nThe University of Texas at Dallas\n$1,000\n\n\n2018\nStudent Travel Award\nThe University of Texas at Dallas\n$1,000\n\n\n2016\nHonorable Mention for Outstanding Service as a Teaching Assistant\nSan Diego State University\n\n\n\n\n2016\nRobert L. Solso Award\nWestern Psychological Association\n$500\n\n\n2015\nDr. Edward Geldreich Scholarship\nSan Diego State University\n$425\n\n\n2015\nRobert L. Solso Award\nWestern Psychological Association\n$500\n\n\n2014 (June)\nTeacher of the Month\nHuntington Learning Center\n\n\n\n\n2014\nStudent Scholarship Award\nWestern Psychological Association\n$500\n\n\n2014 (January)\nTeacher of the Month\nHuntington Learning Center\n\n\n\n\n2012 (Fall)\nDean’s List\nCalifornia State University, Northridge\n\n\n\n\n2011 (Spring)\nDean’s List\nCalifornia State University, Northridge\n\n\n\n\n2011 (Fall)\nDean’s List\nCalifornia State University, Northridge\n\n\n\n\n2010 (Spring)\nDean’s List\nCalifornia State University, Northridge\n\n\n\n\n2010 (September)\nClinical Care Extender Intern of the Rotation Award\nCope Health Solutions\n\n\n\n\n2017 (Fall)\nDean’s List\nCalifornia State University, Northridge\n\n\n\n\nsum\n—\n—\n$4,925"
  },
  {
    "objectID": "cv.html#professional-society-memberships",
    "href": "cv.html#professional-society-memberships",
    "title": "Curriculum Vitae",
    "section": "Professional Society Memberships",
    "text": "Professional Society Memberships\n\n\n\n\n\n\n\n\nYear\nOrganization\n\n\n\n\n2024 - Present\nInternational Society for Tractography\n\n\n2020 - Present\nOrganization for Human Brain Mapping\n\n\n2020 - Present\nCognitive Neuroscience Society\n\n\n2012 - Present\nSociety for Neuroscience\n\n\n2016 - 2017\nPhi Kappa Phi Honor Society\n\n\n2015 - 2017\nThe Obesity Society\n\n\n2012 - 2016\nWestern Psychological Association"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n\n\n\n\nPh.D., Cognition and Neuroscience (2025)  The University of Texas at Dallas  Advisors: Dr. Karen M. Rodrigue & Dr. Kristen M. Kennedy  Dissertation: The effects of age and dopaminergic predisposition on neurite microstructural properties and working memory \n\n\n\n\n\n\n\nM.A., Psychology (2016)  San Diego State University  Advisor: Dr. Claire Murphy  Thesis: Altered functional connectivity and eating disinhibition in individuals with Metabolic Syndrome [url] \n\n\n\n\n\n\n\nB.A., Psychology (2012)  California State University, Northridge  Advisor: Dr. Jose P. Abara"
  },
  {
    "objectID": "cv.html#conference-posters",
    "href": "cv.html#conference-posters",
    "title": "Curriculum Vitae",
    "section": "Conference Posters",
    "text": "Conference Posters\n\n\n\n\n\n\n\n\n\n\nPongpipat, E. E., Rodrigue K. M., & Kennedy, K. M. (2025, February). Cortico-striato-pallido-thalamic loop: Effects of age and dopaminergic predisposition on the spatial gradients of gray-matter neurite microstructural properties.. Poster session presented at the 2025 Dallas Aging and Cognition Conference, Dallas, Texas, United States\nEdwards, V. M., Pongpipat, E. E., Lung, T.-C., Kraft, J. N., Rodrigue, K. M., & Kennedy, K. M. (2025, February). A comparison of automated and semi-automated white matter hyperintensity segmentation techniques.. Poster session presented at the 2025 Dallas Aging and Cognition Conference, Dallas, Texas, United States\nNguyen, L. T., Nair, J. K., Hoagey, D. A., Pongpipat, E. E., Kraft, J. N., Kennedy, K. M., & Rodrigue, K. M. (2025, February). Aging and Alzheimer’s disease risk factors are associated with differential 4-year decline in regional medial temporal lobe volumes. Poster session presented at the 2025 Dallas Aging and Cognition Conference, Dallas, Texas, United States\nLung, T., Pongpipat, E. E., Rodrigue, K. M., & Kennedy, K. M. (2024, June). Longitudinal BOLD Variability Change Coincides with Executive Function Change. Poster session presented at the 2024 Organization for Human Brain Mapping, Seoul, South Korea\nHoagey, D. A., Pongpipat, E. E., Rodrigue, K. M., & Kennedy, K. M. (2023, July). Estimates of structural brain health demonstrate regionally differential coupling in aging that aligns with underlying biology. Poster session presented at the 2023 Organization for Human Brain Mapping, Montreal, Quebec, Canada\nLung, T., Pongpipat, E. E., Rodrigue, K. M., & Kennedy, K. M. (2023, July). Longitudinal changes in fMRI task difficulty modulation: Differential relations to cognitive change. Poster session presented at the 2023 Organization for Human Brain Mapping, Montreal, Quebec, Canada\nEdwards, V. M., Pongpipat, E. E., Miranda, G. G., Kennedy, K. M., & Rodrigue, K. M. (2023, February). Four-year age-related cortical thinning is modified synergistically by inflammation, beta-amyloid, and hypertension. Poster session presented at the 2023 Dallas Aging and Cognition Conference, Dallas, Texas, United States\nHoagey, D. A., Pongpipat, E. E., Rodrigue, K. M., & Kennedy, K. M. (2023, February). White and gray matter structural brain properties demonstrate regionally differential coupling in aging corresponding with underlying biology. Poster session presented at the 2023 Dallas Aging and Cognition Conference, Dallas, Texas, United States\nLung, T., Pongpipat, E. E., Rodrigue, K. M., & Kennedy, K. M. (2023, February). Aging-related differences in BOLD modulation and its relation to cognitive control: A longitudinal fMRI study. Poster session presented at the 2023 Dallas Aging and Cognition Conference, Dallas, Texas, United States\nMiranda, G. G., Pongpipat, E. E., Gonen, C., Rodrigue, K. M., & Kennedy, K. M. (2023, February). Dopaminergic genetic influence on longitudinal change in cortical thickness and age-associated executive function. Poster session presented at the 2023 Dallas Aging and Cognition Conference, Dallas, Texas, United States\nSkotnicki, M. H., Kennedy, K. M., Miranda, G. G., Pongpipat, E. E., & Rodrigue, K. M. (2023, February). Baseline subjective memory complaints predict four-year cortical thinning. Poster session presented at the 2023 Dallas Aging and Cognition Conference, Dallas, Texas, United States\nPongpipat, E. E., Boylan, M. A., Lung, T., Kennedy, K. M., & Rodrigue, K. M. (2022, June). Four-year change in BOLD variability differs by task difficulty, beta-amyloid deposition, and aging. Poster session presented at the 2022 Organization for Human Brain Mapping, Glasgow, Scotland\nPongpipat, E. E., Lung, T., Kennedy, K. M., & Rodrigue, K. M. (2022, January, accepted). Four Year Longitudinal Change in Task-Related BOLD Modulation across the Adult Lifespan. Poster session presented at the 2022 Dallas Aging and Cognition Conference, Dallas, Texas, United States. (Conference cancelled due to COVID-19 omicron variant).\nPongpipat, E. E., Boylan, M. A., Kennedy, K. M., & Rodrigue, K. M. (2022, April). Influence of β-Amyloid and Age on BOLD variability during n-back. Poster session presented at the 2022 Cognitive Aging Conference, Atlanta, Georgia, United States\nKennedy, K. M., Pongpipat, E. E., Boylan, M. A., Lung, T., Rodrigue, K. M. (2022, April). Age-related longitudinal change in BOLD variability during working memory load is dependent upon beta-amyloid deposition. Poster session presented at the 2022 Cognitive Neuroscience Society, San Francisco, California, United States\nPongpipat, E. E., Boylan, M. A., Foster, C. M., Web, C. E., Kennedy, K. K., & Rodrigue, K. M. (2020, June). Which BOLD feature is most important to working memory performance?. Poster session presented at the 2020 Organization for Human Brain Mapping, Virtual\nPongpipat, E. E., Kennedy, K. M., Boylan, M. A., & Rodrigue, K. M. (2020, April, accepted). Which is More Important to n-back Performance: Changes in BOLD or Changes in BOLD Synchronization?. Poster session presented at the 2020 Cognitive Aging Conference, Atlanta, Georgia, United States. (Conference cancelled due to COVID-19).\nBoylan, M. A., Rodrigue, K. M., Webb, C. E., Pongpipat, E. E., & Kennedy, K. M. (2020, April, accepted). Influence of β-Amyloid and Age on BOLD Variability during n-back. Poster session presented at the 2020 Cognitive Aging Conference, Atlanta, Georgia, United States. (Conference cancelled due to COVID-19).\nKennedy, K. M., Foster, C. M., Webb, C. E., Pongpipat, E. E., Boylan, M. A., & Rodrigue, K. M. (2020, April, accepted). Four Year Longitudinal Change in Task-Related Functional Activation across the Adult Lifespan. Poster session presented at the 2020 Cognitive Aging Conference, Atlanta, Georgia, United States. (Conference cancelled due to COVID-19).\nMurphy, C., Pongpipat, E. E., Jacobson, A. (2019, September). Is Eating Disinhibition Associated with Altered Functional Connectivity of the Primary Gustatory Cortex with Secondary Gustatory Cortex, Reward or Memory Regions in Metabolic Syndrome?. Poster session presented at the 2019 Chemical Senses Conference, Bonita Springs, Florida, United States\nKmiecik, M. J., Martin, A. D., Kim, L. M., Perez, R., Martinez, D. M., Pongpipat, E. E., & Krawczyk, D. C. (2019, March). Relational Match-to-Sample in Primates: The Interesting Case of the Human. Poster session presented at the 26th Annual Meeting of the Cognitive Neuroscience Society, San Francisco, California, United States\nHoagey, D. A., Foster, C. M., Pongpipat, E. E., Rodrigue, K. M., & Kennedy, K. M. (2019, June). White matter hyperintensities evidence altered diffusion properties suggestive of poorer white matter health in aging. Poster session presented at the 2019 Organization for Human Brain Mapping Annual Meeting, Rome, Italy\nPongpipat, E. E., Foster, C. M., Boylan, M. A., Kennedy, K. M. & Rodrigue, K. M. (2019, January). Examination of the relationship among n-back functional connectivity, task performance, and cortical thickness across the adult lifespan. Poster session presented at the 2019 Dallas Aging and Cognition Conference, Dallas, Texas, United States\nLeverett, S. D., Boylan, M. A., Pongpipat, E. E., Rodrigue, K. M., Kennedy, K. M (2019, January). Personality, Cognition, and Aging: How and when do personality traits affect executive functioning?. Poster session presented at the 2019 Dallas Aging and Cognition Conference, Dallas, Texas, United States\nPongpipat, E. E., Boylan, M. A., Foster, C. M., Kennedy, K. K. (2018, November). Functional connectivity of the fronto-parietal and default mode network during n-back across the adult lifespan. Poster session presented at the 48th Annual Meeting of the Society for Neuroscience, San Diego, California, United States\nPongpipat, E. E., Jacobson, A. & Murphy, C. (2016, November). Metabolic Syndrome is Associated with Altered Functional Connectivity of Primary and Secondary Taste Cortices and Eating Disinhibition. Poster session presented at the 46th Annual Meeting of the Society for Neuroscience, San Diego, California, United States\nVertrees, R. B., Pongpipat, E. E., McIntosh, E. C., & Murphy, C. (2016, November). Cortical correlates of metabolic syndrome risk factors and hunger in middle aged and older adults. Poster session presented at the 46th Annual Meeting of the Society for Neuroscience, San Diego, California, United States\nPongpipat, E. E., Jacobson, A. & Murphy, C. (2016, April). Metabolic Syndrome is associated with decreased medial temporal lobe cortical thickness and impaired memory ability. Poster session presented at the 96th Annual Convention of the Western Psychological Association, Long Beach, California, United States. (Awarded 2016 Western Psychological Association Student Scholarship Award for Outstanding Quality of Research Presented at the Annual Convention).\nPongpipat, E. E., Jacobson, A. & Murphy, C. (2015, November). Functional Connectivity of Taste Sensory and Pleasantness Pathway in Metabolic Syndrome. Poster session presented at the Annual Convention of the Obesity Society and ObesityWeek, Los Angeles, California, United States\nPongpipat, E. E., & Murphy, C. (2015, April). Effects of Cognitive Decline on Odor Threshold and Identification in Alzheimer’s disease. Poster session presented at the 95th Annual Convention of the Western Psychological Association, Las Vegas, Nevada, United States. (Awarded 2015 Western Psychological Association Student Scholarship Award for Outstanding Quality of Research Presented at the Annual Convention).\nPongpipat, E. E., Magaña, V.M., Sarkissians, S., Camacho, V. & Abara, J.P. (2014, April). An ERP Study of Expectation and Motor Preparation Following Neurofeedback Procedure. Poster session presented at the 94th Annual Convention of the Western Psychological Association, Portland, Oregon, United States. (Awarded 2014 Western Psychological Association Student Scholarship Award for Outstanding Quality of Research Presented at the Annual Convention).\nKelson, C.Y., Pongpipat, E. E., & Abara, J.P. (2014, April). Functional Impact of Neurofeedback on Veterans with PTSD. Poster session presented at the 35th Annual Meeting of the Society of Behavioral Medicine, Philadelphia, Pennsylvania, United States\nNeswald, J., Pongpipat, E. E., Magaña, V.M., Sarkissians, S., Lenik, C., Barb, D., & Abara, J.P. (2013, November). The Amplitude of the Contingent Negative Variation Following Neurofeedback Procedure. Poster session presented at the 43rd Annual Meeting of the Society for Neuroscience, San Diego, California, United States\nMagaña, V.M., Pongpipat, E. E., Rodriguez, T., Camacho, V., Valadez, E.A., & Abara, J.P. (2012, October). The Effects of Cell Phone Use on the P3 of the Event-Related Potential and on Response Time. Poster session presented at the 42nd Annual Meeting of the Society for Neuroscience, New Orleans, Louisiana, United States\nScott, N.A., Magaña, V.M., Pongpipat, E. E., Salehi, N.L., Barb, D., & Abara, J.P. (2012, October). An ERP Study of Inhibitory Response During Cell Phone Use. Poster session presented at the 42nd Annual Meeting of the Society for Neuroscience, New Orleans, Louisiana, United States\nPongpipat, E. E., Magaña, V.M., Camacho, V., Neswald, J., Barb, D., Lenik, C., Castillo, G., Arab, A., & Abara, J.P. (2012, April). Cell Phone Use Delays Inhibitory Response. Poster session presented at the 93rd Annual Convention of the Western Psychological Association, Reno, Nevada, United States"
  },
  {
    "objectID": "cv.html#service-to-the-profession",
    "href": "cv.html#service-to-the-profession",
    "title": "Curriculum Vitae",
    "section": "Service to the Profession",
    "text": "Service to the Profession\n\nReviewer for Psychophysiology\nCo-reviewer with Dr. Kristen Kennedy for:\n\nJournal of Cognitive Neuroscience\nNeuroImage\n\nCo-reviewer with Dr. Karen Rodrigue for:\n\nCerebral Cortex\nJournal of Neuroscience"
  },
  {
    "objectID": "cv.html#research-experience",
    "href": "cv.html#research-experience",
    "title": "Curriculum Vitae",
    "section": "Research Experience",
    "text": "Research Experience\n\n\n\n\n\n\n\n\n\nGraduate Research Assistant (2017 - 2025) Cognitive Neuroscience of Aging Laboratory Center for Vital Longevity School of Behavioral and Brain Sciences The University of Texas at Dallas \n\n\n\n\n\n\n\nGraduate Research Assistant (2014 - 2016) Lifespan Human Senses Laboratory Department of Psychology San Diego State University \n\n\n\n\n\n\n\nUndergraduate Research Assistant (2011 - 2013) Neuroscience Laboratory Department of Psychology California State University, Northridge"
  },
  {
    "objectID": "posts/calculate-motion-regressors/index.html",
    "href": "posts/calculate-motion-regressors/index.html",
    "title": "Calculate 24-Motion Variables",
    "section": "",
    "text": "This is a quick guide to calculate the 24 motion parameters from the original six motion parameters. These 24 motion parameters can be used as control variables (covariates of no interest; nuisance regressors) that is typically performed in fMRI first-level analyses, especially in resting-state and task-based functional connectivity analyses."
  },
  {
    "objectID": "posts/calculate-motion-regressors/index.html#r-libraries",
    "href": "posts/calculate-motion-regressors/index.html#r-libraries",
    "title": "Calculate 24-Motion Variables",
    "section": "1 R Libraries",
    "text": "1 R Libraries\n\n\nCode\n# data loading, manipulation, and writing\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/calculate-motion-regressors/index.html#function",
    "href": "posts/calculate-motion-regressors/index.html#function",
    "title": "Calculate 24-Motion Variables",
    "section": "2 Function",
    "text": "2 Function\nA helper function to plot the motion parameters.\n\n\nCode\n# graph motion parameters and facet by motion type\ngraph_motion &lt;- function(dataset, title) {\n  require(reshape2)\n  data_long &lt;- melt(dataset, id.var = \"Duration\") %&gt;%\n    mutate(variable = gsub(\"_\", \" \", variable))\n  figure &lt;- ggplot(data_long, aes(x = Duration, y = value)) +\n    geom_line(color = '#737373') +\n    theme_minimal() +\n    facet_wrap(~ variable, scales = \"free_y\", ncol = 3) +\n    labs(title = title,\n         y = NULL) +\n    theme(plot.title = element_text(hjust = 0.5))\n  print(figure)\n}"
  },
  {
    "objectID": "posts/calculate-motion-regressors/index.html#rigid-body-motion-regressors",
    "href": "posts/calculate-motion-regressors/index.html#rigid-body-motion-regressors",
    "title": "Calculate 24-Motion Variables",
    "section": "4 6-Rigid Body Motion Regressors",
    "text": "4 6-Rigid Body Motion Regressors\nThe first six motion parameters are the standard motion parameters and include:\n\n\\(t_x\\), translation in the x-direction\n\\(t_y\\), translation in the y-direction\n\\(t_z\\), translation in the z-direction\n\\(\\theta_x\\), rotation around the x-axis\n\\(\\theta_y\\), rotation around the y-axis\n\\(\\theta_z\\), rotation around the z-axis\n\n\n\nCode\n# load 6-rigid body motion parameters\n# add duration\nmotion_file &lt;- \"data/data_demeaned.csv\"\nmotion_demeaned &lt;- read_csv(motion_file, col_names = F) %&gt;%\n  mutate(Duration = c(1:nrow(.)))\n\n# rename columns\nmotion_variables &lt;- c(paste0(\"Translation_\", c(\"X\", \"Y\", \"Z\")),\n                      paste0(\"Rotation_\", c(\"X\", \"Y\", \"Z\")))\ncolnames(motion_demeaned) &lt;- c(motion_variables, \"Duration\")\n\n# graph motion parameters\ngraph_motion(motion_demeaned, \"6-Rigid Body Motion Parameters\")"
  },
  {
    "objectID": "posts/calculate-motion-regressors/index.html#motion-regressors",
    "href": "posts/calculate-motion-regressors/index.html#motion-regressors",
    "title": "Calculate 24-Motion Variables",
    "section": "5 12-Motion Regressors",
    "text": "5 12-Motion Regressors\nThe next six motion parameters are the first temporal derivatives of the original six rigid body motion parameters. Specifically, it is the motion minus the motion from the previous time point.\n\n\\(t_{x_{TD1}}\\), first temporal derivative of the translation in the x-direction\n\\(t_{y_{TD1}}\\), first temporal derivative of the translation in the y-direction\n\\(t_{z_{TD1}}\\), first temporal derivative of the translation in the z-direction\n\\(\\theta_{x_{TD1}}\\) first temporal derivative of the rotation around the x-axis\n\\(\\theta_{y_{TD1}}\\), first temporal derivative of the rotation around the y-axis\n\\(\\theta_{z_{TD1}}\\), first temporal derivative of the rotation around the z-axis\n\n\n\nCode\n# calculate first temporal derivative\nmotion_demeaned_td1 &lt;- motion_demeaned %&gt;%\n  sapply(., FUN = function(x) c(NA, diff(x))) %&gt;%\n  as_tibble() %&gt;%\n  mutate(Duration = c(1:nrow(.)))\n  \n# rename columns\ncolnames(motion_demeaned_td1) &lt;- c(paste0(motion_variables, \"_TD1\"), \"Duration\")\n\n# graph motion parameters\ngraph_motion(motion_demeaned_td1, \"First Temporal Derivatives of the 6-Rigid Body Motion Parameters\")"
  },
  {
    "objectID": "posts/calculate-motion-regressors/index.html#motion-regressors-1",
    "href": "posts/calculate-motion-regressors/index.html#motion-regressors-1",
    "title": "Calculate 24-Motion Variables",
    "section": "6 18-Motion Regressors",
    "text": "6 18-Motion Regressors\nThe next six motion parameters are the original six rigid body motion parameters squared:\n\n\\(t_x^2\\), translation in the x-direction squared\n\\(t_y^2\\), translation in the y-direction squared\n\\(t_z^2\\), translation in the z-direction squared\n\\(\\theta_x^2\\), rotation around the x-axis squared\n\\(\\theta_y^2\\), rotation around the y-axis squared\n\\(\\theta_z^2\\), rotation around the z-axis squared\n\n\n\nCode\n# square 6-rigid body motion parameters\nmotion_demeaned_sq &lt;- motion_demeaned^2 %&gt;%\n  as_tibble() %&gt;%\n  mutate(Duration = c(1:nrow(.)))\n  \n# rename columns\ncolnames(motion_demeaned_sq) &lt;- c(paste0(motion_variables, \"_Squared\"), \"Duration\")\n\n# graph motion parameters\ngraph_motion(motion_demeaned_sq, \"6-Rigid Body Motion Parameters Squared\")"
  },
  {
    "objectID": "posts/calculate-motion-regressors/index.html#motion-regressors-2",
    "href": "posts/calculate-motion-regressors/index.html#motion-regressors-2",
    "title": "Calculate 24-Motion Variables",
    "section": "7 24-Motion Regressors",
    "text": "7 24-Motion Regressors\nThe next six motion parameters are the first temporal derivatives squared.\n\n\\(t_{x_{TD1}}^2\\), first temporal derivative of the translation in the x-direction squared\n\\(t_{y_{TD1}}^2\\), first temporal derivative of the translation in the y-direction squared\n\\(t_{z_{TD1}}^2\\), first temporal derivative of the translation in the z-direction squared\n\\(\\theta_{x_{TD1}}^2\\), first temporal derivative of the rotation around the x-axis squared\n\\(\\theta_{y_{TD1}}^2\\) first temporal derivative of the rotation around the y-axis squared\n\\(\\theta_{z_{TD1}}^2\\), first temporal derivative of the rotation around the z-axis squared\n\n\n\nCode\n# square the first temporal derivative\nmotion_demeaned_td1_sq &lt;- motion_demeaned_td1^2 %&gt;%\n  as_tibble() %&gt;%\n  mutate(Duration = 1:nrow(.)) \n\n# rename columns\ncolnames(motion_demeaned_td1_sq) &lt;- c(paste0(motion_variables, \"_TD1_Squared\"), \"Duration\")\n\n# plot motion.td1.sq into one figure\ngraph_motion(motion_demeaned_td1_sq, \"First Temporal Derivatives of the 6-Rigid Body Motion Parameters Squared\")"
  },
  {
    "objectID": "posts/calculate-motion-regressors/index.html#r-code-example",
    "href": "posts/calculate-motion-regressors/index.html#r-code-example",
    "title": "Calculate 24-Motion Variables",
    "section": "R Code Example",
    "text": "R Code Example\n\n\nCode\n# function to create 24 motion variables =======================================\ncreate_24_motion_variables &lt;- function(dataset) {\n  # mean-center (demean) ----\n  # subtract each score from its respective column mean\n  dataset_motion_demeaned &lt;- scale(x = dataset, center = T, scale = F)\n\n  # first temporal derivatives ----\n  # subtract motion from prior motion time point\n  dataset_motion_td1 &lt;- sapply(dataset_motion_demeaned, FUN = function(x) c(NA, diff(x)))\n\n  # combine the variables into one dataset\n  dataset_motion &lt;- data.frame(dataset_motion_demeaned, dataset_motion_td1)\n\n  # squares ----\n  # square 6-rigid body motion parameters and its temporal derivatives\n  dataset_motion_squared &lt;- dataset_motion^2\n\n  # combine the variables into one dataset\n  dataset_motion &lt;- data.frame(dataset_motion, dataset_motion_squared)\n\n  return(dataset_motion)\n}\n\n# load 6-rigid body motion parameters file =====================================\n# assign to dataset_motion\ndataset_motion &lt;- read_csv(file.choose())\n\n# create 24 motion variables using function ====================================\n# assign output to dataset_motion_24\ndataset_motion_24 &lt;- create_24_motion_variables(dataset = dataset_motion)\n\n# rename columns ===============================================================\n# assign original motion variables\nmotion_variables &lt;- c(\n  \"translation_x\", \"translation_y\", \"translation_z\",\n  \"rotation_x\", \"rotation_y\", \"rotation_z\"\n)\n\n# rename columns\ncolnames(dataset_motion_24) &lt;- c(\n  motion_variables,\n  paste0(motion_variables, \"_td1\"),\n  paste0(motion_variables, \"_sq\"),\n  paste0(motion_variables, \"_td1_sq\")\n)\n\n# export 24-motion variables ===================================================\nwrite_csv(x = dataset_motion_24, file = \"dataset-motion-24.csv\")"
  },
  {
    "objectID": "posts/calculate-motion-regressors/index.html#references",
    "href": "posts/calculate-motion-regressors/index.html#references",
    "title": "Calculate 24-Motion Variables",
    "section": "5 References",
    "text": "5 References\n\nFriston, K. J., Williams, S., Howard, R., Frackowiak, R. S. J., & Turner, R. (1996). Movement-related effects in fMRI time-series. Magnetic Resonance in Medicine, 35(3), 346–355. http://doi.org/10.1002/mrm.1910350312\nYan, C.-G., Cheung, B., Kelly, C., Colcombe, S., Craddock, R. C., Di Martino, A., … Milham, M. P. (2013). A comprehensive assessment of regional variation in the impact of head micromovements on functional connectomics. NeuroImage, 76, 183–201. http://doi.org/10.1016/j.neuroimage.2013.03.004\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/calculate-motion-regressors/index.html#full-r-code-example",
    "href": "posts/calculate-motion-regressors/index.html#full-r-code-example",
    "title": "Calculate 24-Motion Variables",
    "section": "4 Full R Code Example",
    "text": "4 Full R Code Example\n\n\nCode\n# function to create 24 motion variables =======================================\ncreate_24_motion_variables &lt;- function(dataset) {\n  # mean-center (demean) ----\n  # subtract each score from its respective column mean\n  dataset_motion_demeaned &lt;- scale(x = dataset, center = T, scale = F)\n\n  # first temporal derivatives ----\n  # subtract motion from prior motion time point\n  dataset_motion_td1 &lt;- sapply(dataset_motion_demeaned, FUN = function(x) c(NA, diff(x)))\n\n  # combine the variables into one dataset\n  dataset_motion &lt;- data.frame(dataset_motion_demeaned, dataset_motion_td1)\n\n  # squares ----\n  # square 6-rigid body motion parameters and its temporal derivatives\n  dataset_motion_squared &lt;- dataset_motion^2\n\n  # combine the variables into one dataset\n  dataset_motion &lt;- data.frame(dataset_motion, dataset_motion_squared)\n\n  return(dataset_motion)\n}\n\n# load 6-rigid body motion parameters file =====================================\n# assign to dataset_motion\ndataset_motion &lt;- read_csv(file.choose())\n\n# create 24 motion variables using function ====================================\n# assign output to dataset_motion_24\ndataset_motion_24 &lt;- create_24_motion_variables(dataset = dataset_motion)\n\n# rename columns ===============================================================\n# assign original motion variables\nmotion_variables &lt;- c(\n  \"trans_x\", \"trans_y\", \"trans_z\",\n  \"rot_x\", \"rot_y\", \"rot_z\"\n)\n\n# rename columns\ncolnames(dataset_motion_24) &lt;- c(\n  motion_variables,\n  paste0(motion_variables, \"_td1\"),\n  paste0(motion_variables, \"_sq\"),\n  paste0(motion_variables, \"_td1_sq\")\n)\n\n# export 24-motion variables ===================================================\nwrite_csv(x = dataset_motion_24, file = \"dataset-motion-24.csv\")\n\n\n\nNote: Similar logic can also be applied to cerebral spinal fluid (CSF) and white matter (WM) time series to obtain an additional 8 nuisance regressors (32 nuisance regressors in total)."
  },
  {
    "objectID": "posts/solving-ols-regression-using-matrix/index.html",
    "href": "posts/solving-ols-regression-using-matrix/index.html",
    "title": "Solving Ordinary Least Squares (OLS) Regression Using Matrix Algebra",
    "section": "",
    "text": "In psychology, we typically learn how to calculate OLS regression by calculating each coefficient separately. However, I recently learned how to calculate this using matrix algebra. Here is a brief tutorial on how to perform this using R."
  },
  {
    "objectID": "posts/solving-ols-regression-using-matrix/index.html#quarto",
    "href": "posts/solving-ols-regression-using-matrix/index.html#quarto",
    "title": "Solving Ordinary Least Squares (OLS) Regression Using Matrix Algebra",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "posts/solving-ols-regression-using-matrix/index.html#running-code",
    "href": "posts/solving-ols-regression-using-matrix/index.html#running-code",
    "title": "Solving Ordinary Least Squares (OLS) Regression Using Matrix Algebra",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "posts/solving-ols-regression-using-matrix/index.html#r-packages",
    "href": "posts/solving-ols-regression-using-matrix/index.html#r-packages",
    "title": "Solving Ordinary Least Squares (OLS) Regression Using Matrix Algebra",
    "section": "1 R Packages",
    "text": "1 R Packages\n\n\nCode\npackages &lt;- c(\"tidyverse\", \"broom\")\nxfun::pkg_attach(packages, message = F)"
  },
  {
    "objectID": "posts/solving-ols-regression-using-matrix/index.html#dataset",
    "href": "posts/solving-ols-regression-using-matrix/index.html#dataset",
    "title": "Solving Ordinary Least Squares (OLS) Regression Using Matrix Algebra",
    "section": "2 Dataset",
    "text": "2 Dataset\n\n\nCode\ndataset &lt;- carData::Salaries %&gt;%\n  select(salary, yrs.since.phd) %&gt;%\n  mutate(yrs.since.phd = scale(yrs.since.phd, center = T, scale = F))\n\n\n\n\nCode\nsummary(dataset)\n\n\n     salary             yrs.since.phd.V1      \n Min.   : 57800   Min.   :-21.31486146100000  \n 1st Qu.: 91000   1st Qu.:-10.31486146100000  \n Median :107300   Median : -1.31486146096000  \n Mean   :113706   Mean   : -0.00000000000001  \n 3rd Qu.:134185   3rd Qu.:  9.68513853904000  \n Max.   :231545   Max.   : 33.68513853900000  \n\n\nThe Salaries dataset is from the carData package, which shows the salary of professors in the US during the academic year of 2008 and 2009. Let’s say we are interested in determining if professors who have had their Ph.D. degree for longer are more likely to also have higher salaries."
  },
  {
    "objectID": "posts/solving-ols-regression-using-matrix/index.html#solve-using-matrix-algebra",
    "href": "posts/solving-ols-regression-using-matrix/index.html#solve-using-matrix-algebra",
    "title": "Solving Ordinary Least Squares (OLS) Regression Using Matrix Algebra",
    "section": "3 Solve Using Matrix Algebra",
    "text": "3 Solve Using Matrix Algebra\n\n3.1 Design Matrix (\\(X\\))\nThe design matrix is just a dataset of the all the predictors, which includes the intercept set at 1 and yrs.since.phd.\n\n\nCode\nx &lt;- tibble(\n  intercept = 1,\n  yrs.since.phd = as.numeric(dataset$yrs.since.phd)\n) %&gt;%\n  as.matrix()\nhead(x)\n\n\n     intercept yrs.since.phd\n[1,]         1     -3.314861\n[2,]         1     -2.314861\n[3,]         1    -18.314861\n[4,]         1     22.685139\n[5,]         1     17.685139\n[6,]         1    -16.314861\n\n\n\n\n\n3.2 Dependent Variable (\\(Y\\))\n\n\nCode\ny &lt;- dataset$salary %&gt;% as.matrix()\nhead(y)\n\n\n       [,1]\n[1,] 139750\n[2,] 173200\n[3,]  79750\n[4,] 115000\n[5,] 141500\n[6,]  97000\n\n\n\n\n\n3.3 \\(X'X\\)\nFirst, we need to solve for \\(X'X\\), which is the transposed design matrix (\\(X'\\)) multiplied by the design matrix (\\(X\\)).\nLet’s take a look at what \\(X'\\) looks like.\n\n\nCode\nx_transposed &lt;- t(x)\nx_transposed[, 1:6]\n\n\n                   [,1]      [,2]      [,3]     [,4]     [,5]      [,6]\nintercept      1.000000  1.000000   1.00000  1.00000  1.00000   1.00000\nyrs.since.phd -3.314861 -2.314861 -18.31486 22.68514 17.68514 -16.31486\n\n\n\nAfter multiplication, the matrix provides the total number of participants (\\(n\\) = 397; really, the sum of the intercept), sum of yrs.since.phd (\\(\\Sigma(yrs.since.phd)\\) = 0), and sum of squared yrs.since.phd (\\(\\Sigma (yrs.since.phd^2)\\) = 65765.64). Respectively, \\(\\Sigma (years.since.phd)\\) and \\(\\Sigma (yrs.since.phd^2)\\) are sum of error (\\(\\Sigma(yrs.since.phd-M_{yrs.since.phd})\\)) and sum of squared error (\\(\\Sigma(yrs.since.phd-M_{yrs.since.phd})^2\\)) because we first centered the yrs.since.phd variable.\n\n\nCode\nx_prime_x &lt;- (x_transposed %*% x)\nx_prime_x %&gt;% round(., 2)\n\n\n              intercept yrs.since.phd\nintercept           397          0.00\nyrs.since.phd         0      65765.64\n\n\n\nLet’s verify this.\n\n\nCode\ncolSums(x) %&gt;% round(., 2)\n\n\n    intercept yrs.since.phd \n          397             0 \n\n\nCode\ncolSums(x^2) %&gt;% round(., 2)\n\n\n    intercept yrs.since.phd \n       397.00      65765.64 \n\n\n\n\n\n3.4 \\((X'X)^{-1}\\)\n\\((X'X)^{-1}\\) is the inverse matrix of \\(X'X\\).\n\n\nCode\nx_prime_x_inverse &lt;- solve(x_prime_x)\nx_prime_x_inverse\n\n\n                 intercept yrs.since.phd\nintercept     2.518892e-03  9.280150e-20\nyrs.since.phd 9.280150e-20  1.520551e-05\n\n\n\n\n\n3.5 \\(X'Y\\)\n\\(X'Y\\) contains the sum of Y (\\(\\Sigma Y\\) = 45141464) and sum of \\(XY\\) (\\(\\Sigma XY\\) = 64801658).\n\n\nCode\nx_prime_y &lt;- x_transposed %*% y\nx_prime_y\n\n\n                  [,1]\nintercept     45141464\nyrs.since.phd 64801658\n\n\n\nLet’s verify this.\n\n\nCode\nsum(y)\n\n\n[1] 45141464\n\n\nCode\nsum(x[, 2] * y)\n\n\n[1] 64801658\n\n\n\n\n\n3.6 Coefficients (\\(B\\))\nTo obtain the coefficients, we can multiply these last two matrices (\\(B = (X'X)^{-1}X'Y\\)).\n\n\nCode\ncoef &lt;- x_prime_x_inverse %*% x_prime_y\ncoef\n\n\n                     [,1]\nintercept     113706.4584\nyrs.since.phd    985.3421\n\n\n\n\n\n3.7 Standard Error\nTo calculate the standard error, we multiply the inverse matrix of \\(X'X\\) by the mean squared error (MSE) of the model and take the square root of its diagonal matrix (\\(\\sqrt{diag((X'X)^{-1} * MSE)}\\)).\n\nFirst, we need to calculate the \\(MSE\\) of the model. Calculating \\(MSE\\) of the model is still the same, \\(MSE = \\frac{\\Sigma(Y-\\hat{Y})^{2}}{n-p} = \\frac{\\Sigma(e^2)}{df}\\) where \\(Y\\) is the DV, \\(\\hat{Y}\\) is the predicted DV, \\(n\\) is the total number of participants (or data points), and \\(p\\) is the total number of variables in the design matrix (or predictors, which includes the intercept).\n\nTo obtain the predicted values (\\(\\hat{Y}\\)), we can also use matrix algebra by multiplying the design matrix with the coefficients (\\(\\hat{Y} = XB\\)).\n\n\nCode\ny_predicted &lt;- x %*% coef\nhead(y_predicted)\n\n\n          [,1]\n[1,] 110440.19\n[2,] 111425.53\n[3,]  95660.05\n[4,] 136059.08\n[5,] 131132.37\n[6,]  97630.74\n\n\n\nNow that we have \\(\\hat{Y}\\), we can then calculate the \\(MSE\\).\n\n\nCode\ne &lt;- y - y_predicted\nse &lt;- sum(e^2)\nn &lt;- nrow(x)\np &lt;- ncol(x)\ndf &lt;- n - p\nmse &lt;- se / df\nmse\n\n\n[1] 758098328\n\n\n\nThen, we multiply \\((X'X)^{-1}\\) by MSE.\n\n\nCode\nmse_coef &lt;- x_prime_x_inverse * mse\nmse_coef %&gt;% round(., 2)\n\n\n              intercept yrs.since.phd\nintercept       1909568          0.00\nyrs.since.phd         0      11527.27\n\n\n\nThen, we take the square root of the diagonal matrix to obtain the standard error of the coefficients.\n\n\nCode\nrmse_coef &lt;- sqrt(diag(mse_coef))\nrmse_coef %&gt;% round(., 2)\n\n\n    intercept yrs.since.phd \n      1381.87        107.37 \n\n\n\n\n\n3.8 t-Statistic\nThe t-statistic is just the coefficient divided by the standard error of the coefficient.\n\n\nCode\nt_statistic &lt;- as.numeric(coef) / as.numeric(rmse_coef)\nt_statistic\n\n\n[1] 82.284421  9.177488\n\n\n\n\n\n3.9 p-Value\nWe want the probability of obtaining that score or more extreme and not the other way around. Thus, we need to set lower to FALSE. Also, we need to multiply it by 2 to obtain a two-tailed test.\n\n\nCode\np_value &lt;- 2 * pt(t_statistic, df, lower = FALSE)\np_value\n\n\n[1] 1.070665e-250  2.495042e-18\n\n\n\n\n\n3.10 Summary\n\n\nCode\ntibble(\n  term = colnames(x),\n  estimate = as.numeric(coef),\n  std.error = as.numeric(rmse_coef),\n  statistic = as.numeric(t_statistic),\n  p.value = as.numeric(p_value)\n)\n\n\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 intercept      113706.     1382.     82.3  1.07e-250\n2 yrs.since.phd     985.      107.      9.18 2.50e- 18"
  },
  {
    "objectID": "posts/solving-ols-regression-using-matrix/index.html#solve-using-lm-function",
    "href": "posts/solving-ols-regression-using-matrix/index.html#solve-using-lm-function",
    "title": "Solving Ordinary Least Squares (OLS) Regression Using Matrix Algebra",
    "section": "4 Solve Using lm Function",
    "text": "4 Solve Using lm Function\n\n\nCode\nlm(salary ~ yrs.since.phd, dataset) %&gt;% tidy()\n\n\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    113706.     1382.     82.3  1.07e-250\n2 yrs.since.phd     985.      107.      9.18 2.50e- 18\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/calculate-motion-regressors/index.html#calculate-24-motion-parameters",
    "href": "posts/calculate-motion-regressors/index.html#calculate-24-motion-parameters",
    "title": "Calculate 24-Motion Variables",
    "section": "3 Calculate 24-Motion Parameters",
    "text": "3 Calculate 24-Motion Parameters\n\n3.1 6-Rigid Body Motion Regressors\nThe first six motion parameters are the standard motion parameters and include:\n\n\\(t_x\\), translation in the x-direction\n\\(t_y\\), translation in the y-direction\n\\(t_z\\), translation in the z-direction\n\\(\\theta_x\\), rotation around the x-axis\n\\(\\theta_y\\), rotation around the y-axis\n\\(\\theta_z\\), rotation around the z-axis\n\n\n\nCode\n# load 6-rigid body motion parameters\n# add duration\nmotion_file &lt;- \"data/data_demeaned.csv\"\nmotion_demeaned &lt;- read_csv(motion_file, col_names = F) %&gt;%\n  mutate(Duration = c(1:nrow(.)))\n\n# rename columns\nmotion_variables &lt;- c(paste0(\"Translation_\", c(\"X\", \"Y\", \"Z\")),\n                      paste0(\"Rotation_\", c(\"X\", \"Y\", \"Z\")))\ncolnames(motion_demeaned) &lt;- c(motion_variables, \"Duration\")\n\n# graph motion parameters\ngraph_motion(motion_demeaned, \"6-Rigid Body Motion Parameters\")\n\n\n\n\n\n\n\n\n\n\n\n\n3.2 12-Motion Regressors\nThe next six motion parameters are the first temporal derivatives of the original six rigid body motion parameters. Specifically, it is the motion minus the motion from the previous time point.\n\n\\(t_{x_{TD1}}\\), first temporal derivative of the translation in the x-direction\n\\(t_{y_{TD1}}\\), first temporal derivative of the translation in the y-direction\n\\(t_{z_{TD1}}\\), first temporal derivative of the translation in the z-direction\n\\(\\theta_{x_{TD1}}\\) first temporal derivative of the rotation around the x-axis\n\\(\\theta_{y_{TD1}}\\), first temporal derivative of the rotation around the y-axis\n\\(\\theta_{z_{TD1}}\\), first temporal derivative of the rotation around the z-axis\n\n\n\nCode\n# calculate first temporal derivative\nmotion_demeaned_td1 &lt;- motion_demeaned %&gt;%\n  sapply(., FUN = function(x) c(NA, diff(x))) %&gt;%\n  as_tibble() %&gt;%\n  mutate(Duration = c(1:nrow(.)))\n  \n# rename columns\ncolnames(motion_demeaned_td1) &lt;- c(paste0(motion_variables, \"_TD1\"), \"Duration\")\n\n# graph motion parameters\ngraph_motion(motion_demeaned_td1, \"First Temporal Derivatives of the 6-Rigid Body Motion Parameters\")\n\n\n\n\n\n\n\n\n\n\n\n\n3.3 18-Motion Regressors\nThe next six motion parameters are the original six rigid body motion parameters squared:\n\n\\(t_x^2\\), translation in the x-direction squared\n\\(t_y^2\\), translation in the y-direction squared\n\\(t_z^2\\), translation in the z-direction squared\n\\(\\theta_x^2\\), rotation around the x-axis squared\n\\(\\theta_y^2\\), rotation around the y-axis squared\n\\(\\theta_z^2\\), rotation around the z-axis squared\n\n\n\nCode\n# square 6-rigid body motion parameters\nmotion_demeaned_sq &lt;- motion_demeaned^2 %&gt;%\n  as_tibble() %&gt;%\n  mutate(Duration = c(1:nrow(.)))\n  \n# rename columns\ncolnames(motion_demeaned_sq) &lt;- c(paste0(motion_variables, \"_Squared\"), \"Duration\")\n\n# graph motion parameters\ngraph_motion(motion_demeaned_sq, \"6-Rigid Body Motion Parameters Squared\")\n\n\n\n\n\n\n\n\n\n\n\n\n3.4 24-Motion Regressors\nThe next six motion parameters are the first temporal derivatives squared.\n\n\\(t_{x_{TD1}}^2\\), first temporal derivative of the translation in the x-direction squared\n\\(t_{y_{TD1}}^2\\), first temporal derivative of the translation in the y-direction squared\n\\(t_{z_{TD1}}^2\\), first temporal derivative of the translation in the z-direction squared\n\\(\\theta_{x_{TD1}}^2\\), first temporal derivative of the rotation around the x-axis squared\n\\(\\theta_{y_{TD1}}^2\\) first temporal derivative of the rotation around the y-axis squared\n\\(\\theta_{z_{TD1}}^2\\), first temporal derivative of the rotation around the z-axis squared\n\n\n\nCode\n# square the first temporal derivative\nmotion_demeaned_td1_sq &lt;- motion_demeaned_td1^2 %&gt;%\n  as_tibble() %&gt;%\n  mutate(Duration = 1:nrow(.)) \n\n# rename columns\ncolnames(motion_demeaned_td1_sq) &lt;- c(paste0(motion_variables, \"_TD1_Squared\"), \"Duration\")\n\n# plot motion.td1.sq into one figure\ngraph_motion(motion_demeaned_td1_sq, \"First Temporal Derivatives of the 6-Rigid Body Motion Parameters Squared\")"
  },
  {
    "objectID": "posts/blog-solving-ols-regression-using-gradient-descent/index.html",
    "href": "posts/blog-solving-ols-regression-using-gradient-descent/index.html",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "",
    "text": "Already familiar with OLS regression and wanted to learn about gradient descent? This blog post will provide a brief tutorial on solving OLS using gradient descent using R.\nRemember that the equation for OLS is:\n\\[\n\\tag{1}\nY = X\\beta + \\epsilon\n\\]\nwhere \\(Y\\) is a column-wise vector of DVs, \\(X\\) is a matrix of IVs, \\(\\beta\\) is a column-wise vector of regression coefficients, and \\(\\epsilon\\) is a column-wise vector of error.\n\\[\n\\begin{bmatrix}\n\\tag{2}\ny_1 \\\\\ny_2 \\\\\n. \\\\\n. \\\\\n. \\\\\ny_n \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & x_{1, 1} & x_{1, 2} & . & . & . & x_{1, n} \\\\\n1 & x_{2, 1} & x_{2, 2} & . & . & . & x_{2, n}\\\\\n. & . & . & . & . & . & .\\\\\n. & . & . & . & . & . & .\\\\\n. & . & . & . & . & . & .\\\\\n1 & x_{n, 1} & x_{n, 2} & . & . & . & x_{n, n}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nb_0 \\\\\nb_1 \\\\\n. \\\\\n. \\\\\n. \\\\\nb_n \\\\\n\\end{bmatrix} +\n\\begin{bmatrix}\ne_1 \\\\\ne_2 \\\\\n. \\\\\n. \\\\\n. \\\\\ne_n \\\\\n\\end{bmatrix}\n\\]\nFor a review of matrix multiplication, check out my previous blog post.\nOLS tries to minimize error using the mean squared error (MSE) formula:\n\\[\n\\tag{3}\nMSE = \\frac{\\Sigma (Y-\\hat{Y})^2}{N} = \\frac{\\Sigma e_i^2}{N} = mean(e_i^2)\n\\]\nNote: MSE is a single cost function (or formula) from many that we could choose from. For example, another cost function is mean absolute error (MAE):\n\\[\n\\tag{4}\nMAE = \\frac{\\Sigma |Y-\\hat{Y}|}{N} = \\frac{\\Sigma|e_i|}{N} = mean(abs(e_i))\n\\]\nCode\nobtain_cost &lt;- function(y, x, b) {\n  y_pred &lt;- x %*% b\n  e &lt;- y - y_pred\n  se &lt;- e^2\n  mse &lt;- mean(se)\n  return(mse)\n}"
  },
  {
    "objectID": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#r-libraries",
    "href": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#r-libraries",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "1 R Libraries",
    "text": "1 R Libraries\nFirst, let’s load some packages.\n\n\nCode\npackages &lt;- c(\"tidyverse\",    # data manipulation and visualization\n              \"carData\",      # data set to obtain professor's 9-month salary\n              \"broom\",        # nice table format of cofficients from lm()\n              \"ggrepel\",      # ggplot extension to repel text and extra features\n              \"glue\",         # concatenate strings and alias\n              \"RColorBrewer\", # load color palettes\n              \"plotly\"        # plot 3D figures\n              )\nxfun::pkg_attach2(packages, message = F)\nn_round &lt;- 3"
  },
  {
    "objectID": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#data",
    "href": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#data",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "2 Data",
    "text": "2 Data\nThen, let’s define our independent variable (IV: yrs.since.phd) as x and our dependent variable (DV: salary) as y using the Salaries dataset in the carData package.\n\n\nCode\nx &lt;- model.matrix(~ scale(yrs.since.phd, scale = F), Salaries)\ncolnames(x) &lt;- c(\"(Intercept)\", \"yrs.since.phd\")\ndim(x)\n\n\n[1] 397   2\n\n\nCode\nhead(x)\n\n\n  (Intercept) yrs.since.phd\n1           1         -3.31\n2           1         -2.31\n3           1        -18.31\n4           1         22.69\n5           1         17.69\n6           1        -16.31\n\n\nCode\ny &lt;- Salaries$salary %&gt;% as.matrix()\ncolnames(y) &lt;- c(\"salary\")\ndim(y)\n\n\n[1] 397   1\n\n\nCode\nhead(y)\n\n\n     salary\n[1,] 139750\n[2,] 173200\n[3,]  79750\n[4,] 115000\n[5,] 141500\n[6,]  97000"
  },
  {
    "objectID": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#gradient-descent",
    "href": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#gradient-descent",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "3 Gradient Descent",
    "text": "3 Gradient Descent\nWhat is gradient descent?\nStarting at any position (e.g., intercept and slope combination), gradient descent takes the partial derivative of each coefficient (\\(\\beta\\)) from the cost function (MSE) and moves (or descends) in the direction that will continue to minimize the cost (or gradient) function.\n\n\nCode\n# random sample of possible intercepts and slopes\n# then calculate cost function (mse) for each intercept and slope combination\nn_sample &lt;- 200000\ndf_gd &lt;- tibble(\n  int = sample(seq(0, 200000), n_sample, T),\n  slope = sample(seq(-10000, 10000), n_sample, T)\n) %&gt;%\n  rowwise() %&gt;%\n  mutate(cost = obtain_cost(y, x, b = c(int, slope)))\n\n\n\n\nCode\nplot_ly(data = df_gd, x = ~slope, y = ~int, z = ~cost, \n        color = ~cost, colors = c(\"#08306b\", \"#f7fbff\"))\n\n\n\n\n\n\n\n\nCode\n# plot intercept and slope, and color by cost (mse)\n# highlight and label min(cost)\nggplot(df_gd, aes(slope, int, color = cost)) +\n  geom_point() +\n  geom_point(data = subset(df_gd, cost == min(cost)), color = \"white\", shape = 21, alpha = .5) +\n  geom_text_repel(\n    data = subset(df_gd, cost == min(cost)),\n    mapping = aes(label = paste0(\"min(cost) = \", round(cost, n_round), \"\\nintercept = \", round(int, n_round), \"\\nslope = \", round(slope, n_round))),\n    nudge_y = 30000,\n    nudge_x = 1000,\n    box.padding = 1.5,\n    point.padding = 0.5,\n    segment.size = 0.2,\n    segment.color = \"white\",\n    color = \"white\"\n  ) +\n  labs(\n    title = \"Gradient Descent (2D View)\",\n    y = \"intercept\",\n    color = \"cost (mse)\"\n  ) +\n  scale_color_distiller(palette = \"Blues\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe can see from these random samples of intercepts and slopes that the lowest cost is 754436739.905 with an intercept of 113550 and slope of 957. So no matter where we start (any intercept and slope combination), we should descend and ultimately end up with the lowest cost value of 754436739.905."
  },
  {
    "objectID": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#partial-derivatives",
    "href": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#partial-derivatives",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "4 Partial Derivatives",
    "text": "4 Partial Derivatives\nWhat are partial derivatives?\nPartial derivatives allow us to obtain a slope at any point that is specific to a variable on any function (or line). In our case, partial derivatives allow us to obtain a slope at any specific \\(\\beta\\) on the cost function of MSE, which can be denoted as \\(\\frac{\\partial MSE}{\\partial \\hat{\\beta}}\\).\n\nHow do we calculate the partial derivatives?\nFirst, we expand MSE.\n\\[\n\\tag{5}\n\\begin{align}\nMSE & = \\frac{(Y-X\\beta)^2}{N} \\\\\n& = \\frac{(Y-X\\beta)(Y-X\\beta)}{N} \\\\\n& = \\frac{(Y^2-2YX\\beta+X^2\\beta^2)}{N} \\\\\n& = \\frac{Y^2}{N} - \\frac{2YX\\beta}{N}+\\frac{X^2\\beta^2}{N} \\\\\n\\end{align}\n\\]\nTo calculate the partial derivative, we repeat the following for each term:\n\nSet the term without \\(\\beta\\) to 0\nMultiply the term by the exponent of \\(\\beta\\)\nSubtract 1 from the exponent of \\(\\beta\\)\n\nand simplify.\n\\[\n\\tag{6}\n\\begin{align}\n\\frac{\\partial MSE}{\\partial \\hat{\\beta}} & = 0 - \\frac{1*2YX\\beta^0}{N} + \\frac{2X^2\\beta^1}{N} \\\\\n& = \\frac{-2YX+2X^2\\beta}{N} \\\\\n& = \\frac{-2X(Y-X\\beta)}{N} \\\\\n& = \\frac{-2X\\epsilon}{N}\n\\end{align}\n\\] Note: We set the term without \\(\\beta\\) to 0 in step 1 because that term can be thought of as having \\(\\beta^0\\), which equals 1. When we then multiply the term by the exponent of 0, we end up with a term of 0."
  },
  {
    "objectID": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#update-coefficients",
    "href": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#update-coefficients",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "5 Update Coefficients",
    "text": "5 Update Coefficients\nWe can then use the information from partial derivatives to update the coefficients (\\(\\beta\\)) so that coefficients (\\(\\beta\\)) change and descend into a lower MSE.\n\nHow do we update the coefficients?\nWe can update the coefficients \\(\\beta\\) by subtracting the partial derivatives multiplied by the learning rate.\n\\[\n\\tag{7}\n\\hat{\\beta} = \\hat{\\beta} - \\frac{\\partial MSE}{\\partial \\hat{\\beta}}*learning\\ rate\n\\]\nA \\(\\beta\\) lower than it should be will have a negative slope and when we subtract the partial derivative (\\(\\frac{\\partial MSE}{\\partial \\hat{\\beta}}\\)) from the old \\(\\beta\\), the new \\(\\beta\\) will then be less negative. Conversely, a \\(\\beta\\) higher than should be will have a positive slope and when we subtract the partial derivative (\\(\\frac{\\partial MSE}{\\partial \\hat{\\beta}}\\)), the new \\(\\beta\\) will be less positive. When we are at the lowest cost value, the partial derivative (\\(\\frac{\\partial MSE}{\\partial \\hat{\\beta}}\\)) will be 0 and \\(\\beta\\) will stop updating.\n\nWhat is the learning rate?\nThe learning rate determines how fast the coefficients update (descends). A higher learning rate descends quickly but may be susceptible to skipping or moving past the global minima. A lower learning rate is more precise but is slower as it takes more time to compute.\n\n\nCode\nupdate_b &lt;- function(y, x, b, lr) {\n  y_pred &lt;- x %*% b\n  e &lt;- y - y_pred\n  derivatives &lt;- (-2 * t(x) %*% e ) / nrow(x)\n  b &lt;- b - derivatives * lr\n  return(b)\n}"
  },
  {
    "objectID": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#train",
    "href": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#train",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "6 Train",
    "text": "6 Train\nNow let’s train the data, arbitrarily starting the coefficients at 0 and a learning rate of 0.0001 for 50,000 iterations.\n\n\nCode\n# set number of iterations\niter &lt;- 50000\n\n# set learning rate\nlr &lt;- 0.0001\n\n# set initial values of coefficients\nb &lt;- NULL\nfor (i in 1:ncol(x)) {\n  b[i] &lt;- 0\n}\nb &lt;- as.matrix(b)\n\ncat(paste(\"iteration\", \"intercept\", \"slope\", \"cost\\n\", \n          \"0\", round(b[1], n_round), round(b[2], n_round), round(obtain_cost(y, x, b), n_round)))\n\n\niteration intercept slope cost\n 0 0 0 13844273659.244\n\n\nCode\n# set initial training dataset\ndf_train &lt;- tibble(\n  iter = NA,\n  int = NA,\n  slope = NA,\n  cost = NA,\n  .rows = iter\n)\n\n# train and save training history using for loop\nfor (i in 1:iter) {\n  b &lt;- update_b(y, x, b, lr)\n  df_train$iter[i] &lt;- i\n  df_train$int[i] &lt;- b[1]\n  df_train$slope[i] &lt;- b[2]\n  df_train$cost[i] &lt;- obtain_cost(y, x, b)\n  if (i %in% round(seq(1, iter, length.out = 10), 0)) {\n    cat(paste(\"\\n\", round(i, n_round), round(b[1], n_round), round(b[2], n_round), round(obtain_cost(y, x, b), n_round)))\n  }\n}\n\n\n\n 1 22.741 32.646 13828621661.101\n 5556 76282.576 985.342 2154826164.687\n 11112 101389.243 985.342 905992994.67\n 16667 109651.717 985.342 770720117.435\n 22223 112371.933 985.342 756060150.645\n 27778 113267.142 985.342 754472191.632\n 33334 113561.867 985.342 754300099.289\n 38889 113658.86 985.342 754281458.348\n 44445 113690.793 985.342 754279438.168\n 50000 113701.301 985.342 754279219.343\n\n\n\n\n6.1 Plot Training\n\n\nCode\nggplot(df_train, aes(iter, cost)) +\n  geom_line() +\n  theme_minimal() +\n  labs(\n    x = \"iterations\",\n    y = \"cost (mse)\"\n  )\n\n\n\n\n\n\n\n\n\nWe can see that cost (MSE) flattens out around 15,000 iterations.\n\n\n\n6.2 Plot Gradient Descent Path\nLet’s visualize the path that the gradient descent took, starting from our initial intercept and slope value of 0.\n\n\nCode\nn_sample &lt;- 100000\ndf_gd &lt;- tibble(\n  int = sample(seq(0, 150000), n_sample, T),\n  slope = sample(seq(0, 2000), n_sample, T)\n) %&gt;%\n  rowwise() %&gt;%\n  mutate(cost = obtain_cost(y, x, b = c(int, slope)))\n\ndf_train_sorted &lt;- df_train %&gt;% arrange(cost, iter)\ndf_min &lt;- df_train_sorted[1, ]\n\nggplot(df_gd, aes(slope, int, color = cost)) +\n  geom_point() +\n  scale_color_distiller(palette = \"Blues\") +\n  theme_minimal() +\n  labs(\n    y = \"intercept\",\n    color = \"cost (mse)\"\n  ) +\n  geom_line(data = df_train, mapping = aes(slope, int), color = \"black\", size = 0.5, alpha = 0.5) +\n  geom_point(data = df_min, mapping = aes(slope, int), color = \"white\", shape = 21, alpha = .5) +\n  geom_text_repel(\n    data = df_min,\n    mapping = aes(label = paste0(\"min(cost) = \", round(cost, n_round), \"\\nintercept = \", round(int, n_round), \"\\nslope = \", round(slope, n_round))),\n    nudge_y = 30000,\n    nudge_x = 1000,\n    box.padding = 1.5,\n    point.padding = 0.5,\n    segment.size = 0.2,\n    segment.color = \"white\",\n    color = \"white\"\n  )\n\n\n\n\n\n\n\n\n\nWe can see that most of the iterations were spent optimizing the slope with minimal changes to the intercept as well as the cost (MSE) up until around iteration 1000. Then, the optimal intercept was found within a few iterations, which dramatically reduced cost (MSE)."
  },
  {
    "objectID": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#compare-gradient-descent-and-lm",
    "href": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#compare-gradient-descent-and-lm",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "7 Compare Gradient Descent and lm()",
    "text": "7 Compare Gradient Descent and lm()\nWe can compare the results of gradient descent with lm() to ensure that we did this correctly. We should approximately have the same results within rounding error because OLS is a convex shape with only one global minima.\n\n\n7.1 Solve OLS using Gradient Descent\n\n\nCode\ncoef &lt;- c(\"(Intercept)\" = df_min$int, \"yrs.since.phd\" = df_min$slope)\nrmse_coef &lt;- (solve(t(x) %*% x) * df_min$cost) %&gt;%\n  diag() %&gt;%\n  sqrt()\nt_stat &lt;- coef / rmse_coef\nn &lt;- nrow(x)\np &lt;- ncol(x)\ndf &lt;- n - p\np_value &lt;- 2 * pt(t_stat, df, lower = FALSE)\ntibble(\n  term = colnames(x),\n  estimate = coef,\n  std.error = rmse_coef,\n  statistic = t_stat,\n  p.value = p_value\n)\n\n\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    113701.     1378.     82.5  4.24e-251\n2 yrs.since.phd     985.      107.      9.20 2.09e- 18\n\n\n\n\n\n7.2 Solve OLS using lm()\n\n\nCode\nmodel &lt;- lm(y ~ 0 + x)\ntidy(model)\n\n\n# A tibble: 2 × 5\n  term           estimate std.error statistic   p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 x(Intercept)    113706.     1382.     82.3  1.07e-250\n2 xyrs.since.phd     985.      107.      9.18 2.50e- 18\n\n\n\n\n\n7.3 Cost\n\n\nCode\nglue(\"Cost using GD: {round(df_min$cost, n_round)}\")\n\n\nCost using GD: 754279219.343\n\n\nCode\nglue(\"Cost using lm(): {round(glance(model)$sigma^2, n_round)}\")\n\n\nCost using lm(): 758098327.9\n\n\nWe can see that solving OLS regression using gradient descent produced a lower cost function (MSE) than by using lm(). The lower MSE using gradient descent was produced from using a smaller intercept. Since MSE was smaller using gradient descent, the standard error was also smaller using gradient descent and subsequently a larger statistic and lower p-value.\n\n\n\n7.4 Figure\n\n\nCode\nfig_gd &lt;- ggplot(mapping = aes(x[, 2], y)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = coef[1], slope = coef[2], size = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Solved using Gradient Descent\",\n    x = \"yrs.since.phd\",\n    y = \"salary\"\n  )\n\nfig_lm &lt;- ggplot(mapping = aes(x[, 2], y)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(color = \"black\", method = \"lm\", se = F) +\n  theme_minimal() +\n  labs(\n    title = \"Solved using lm()\",\n    x = \"yrs.since.phd\",\n    y = \"salary\"\n  )\n\ngridExtra::grid.arrange(fig_gd, fig_lm, nrow = 1)\n\n\n\n\n\n\n\n\n\nHowever, since the range of salary is so large, each method produced nearly identical graphs."
  },
  {
    "objectID": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#acknowledgements",
    "href": "posts/blog-solving-ols-regression-using-gradient-descent/index.html#acknowledgements",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "8 Acknowledgements",
    "text": "8 Acknowledgements\n\n@bfortuner for his gradient descent in python tutorial.\n@mychan24 for her helpful feedback and suggestions to this blog.\n@praxling for her proofing and feedback to this blog.\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/boot-perm-dash/index.html",
    "href": "posts/boot-perm-dash/index.html",
    "title": "Bootstrapping and Permutation Testing: A Shiny App",
    "section": "",
    "text": "If you are not redirected, please click here"
  },
  {
    "objectID": "posts/multilevel-modeling-in-r-with-nhl-power-play-data/index.html",
    "href": "posts/multilevel-modeling-in-r-with-nhl-power-play-data/index.html",
    "title": "Multilevel Modeling in R with NHL Power Play Data",
    "section": "",
    "text": "If you are not redirected, please click here"
  },
  {
    "objectID": "posts/solving-ols-regression-using-gradient-descent/index.html",
    "href": "posts/solving-ols-regression-using-gradient-descent/index.html",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "",
    "text": "Already familiar with OLS regression and wanted to learn about gradient descent? This blog post will provide a brief tutorial on solving OLS using gradient descent using R.\nRemember that the equation for OLS is:\n\\[\n\\tag{1}\nY = X\\beta + \\epsilon\n\\]\nwhere \\(Y\\) is a column-wise vector of DVs, \\(X\\) is a matrix of IVs, \\(\\beta\\) is a column-wise vector of regression coefficients, and \\(\\epsilon\\) is a column-wise vector of error.\n\\[\n\\begin{bmatrix}\n\\tag{2}\ny_1 \\\\\ny_2 \\\\\n. \\\\\n. \\\\\n. \\\\\ny_n \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & x_{1, 1} & x_{1, 2} & . & . & . & x_{1, n} \\\\\n1 & x_{2, 1} & x_{2, 2} & . & . & . & x_{2, n}\\\\\n. & . & . & . & . & . & .\\\\\n. & . & . & . & . & . & .\\\\\n. & . & . & . & . & . & .\\\\\n1 & x_{n, 1} & x_{n, 2} & . & . & . & x_{n, n}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nb_0 \\\\\nb_1 \\\\\n. \\\\\n. \\\\\n. \\\\\nb_n \\\\\n\\end{bmatrix} +\n\\begin{bmatrix}\ne_1 \\\\\ne_2 \\\\\n. \\\\\n. \\\\\n. \\\\\ne_n \\\\\n\\end{bmatrix}\n\\]\nFor a review of matrix multiplication, check out my previous blog post.\nOLS tries to minimize error using the mean squared error (MSE) formula:\n\\[\n\\tag{3}\nMSE = \\frac{\\Sigma (Y-\\hat{Y})^2}{N} = \\frac{\\Sigma e_i^2}{N} = mean(e_i^2)\n\\]\nNote: MSE is a single cost function (or formula) from many that we could choose from. For example, another cost function is mean absolute error (MAE):\n\\[\n\\tag{4}\nMAE = \\frac{\\Sigma |Y-\\hat{Y}|}{N} = \\frac{\\Sigma|e_i|}{N} = mean(abs(e_i))\n\\]\nCode\nobtain_cost &lt;- function(y, x, b) {\n  y_pred &lt;- x %*% b\n  e &lt;- y - y_pred\n  se &lt;- e^2\n  mse &lt;- mean(se)\n  return(mse)\n}"
  },
  {
    "objectID": "posts/solving-ols-regression-using-gradient-descent/index.html#r-libraries",
    "href": "posts/solving-ols-regression-using-gradient-descent/index.html#r-libraries",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "1 R Libraries",
    "text": "1 R Libraries\nFirst, let’s load some packages.\n\n\nCode\npackages &lt;- c(\"tidyverse\",    # data manipulation and visualization\n              \"carData\",      # data set to obtain professor's 9-month salary\n              \"broom\",        # nice table format of cofficients from lm()\n              \"ggrepel\",      # ggplot extension to repel text and extra features\n              \"glue\",         # concatenate strings and alias\n              \"RColorBrewer\", # load color palettes\n              \"plotly\"        # plot 3D figures\n              )\nxfun::pkg_attach2(packages, message = F)\nn_round &lt;- 3"
  },
  {
    "objectID": "posts/solving-ols-regression-using-gradient-descent/index.html#data",
    "href": "posts/solving-ols-regression-using-gradient-descent/index.html#data",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "2 Data",
    "text": "2 Data\nThen, let’s define our independent variable (IV: yrs.since.phd) as x and our dependent variable (DV: salary) as y using the Salaries dataset in the carData package.\n\n\nCode\nx &lt;- model.matrix(~ scale(yrs.since.phd, scale = F), Salaries)\ncolnames(x) &lt;- c(\"(Intercept)\", \"yrs.since.phd\")\ndim(x)\n\n\n[1] 397   2\n\n\nCode\nhead(x)\n\n\n  (Intercept) yrs.since.phd\n1           1         -3.31\n2           1         -2.31\n3           1        -18.31\n4           1         22.69\n5           1         17.69\n6           1        -16.31\n\n\nCode\ny &lt;- Salaries$salary %&gt;% as.matrix()\ncolnames(y) &lt;- c(\"salary\")\ndim(y)\n\n\n[1] 397   1\n\n\nCode\nhead(y)\n\n\n     salary\n[1,] 139750\n[2,] 173200\n[3,]  79750\n[4,] 115000\n[5,] 141500\n[6,]  97000"
  },
  {
    "objectID": "posts/solving-ols-regression-using-gradient-descent/index.html#gradient-descent",
    "href": "posts/solving-ols-regression-using-gradient-descent/index.html#gradient-descent",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "3 Gradient Descent",
    "text": "3 Gradient Descent\nWhat is gradient descent?\nStarting at any position (e.g., intercept and slope combination), gradient descent takes the partial derivative of each coefficient (\\(\\beta\\)) from the cost function (MSE) and moves (or descends) in the direction that will continue to minimize the cost (or gradient) function.\n\n\nCode\n# random sample of possible intercepts and slopes\n# then calculate cost function (mse) for each intercept and slope combination\nn_sample &lt;- 200000\ndf_gd &lt;- tibble(\n  int = sample(seq(0, 200000), n_sample, T),\n  slope = sample(seq(-10000, 10000), n_sample, T)\n) %&gt;%\n  rowwise() %&gt;%\n  mutate(cost = obtain_cost(y, x, b = c(int, slope)))\n\n\n\n\nCode\nplot_ly(data = df_gd, x = ~slope, y = ~int, z = ~cost, \n        color = ~cost, colors = c(\"#08306b\", \"#f7fbff\"))\n\n\n\n\n\n\n\n\nCode\n# plot intercept and slope, and color by cost (mse)\n# highlight and label min(cost)\nggplot(df_gd, aes(slope, int, color = cost)) +\n  geom_point() +\n  geom_point(data = subset(df_gd, cost == min(cost)), color = \"white\", shape = 21, alpha = .5) +\n  geom_text_repel(\n    data = subset(df_gd, cost == min(cost)),\n    mapping = aes(label = paste0(\"min(cost) = \", round(cost, n_round), \"\\nintercept = \", round(int, n_round), \"\\nslope = \", round(slope, n_round))),\n    nudge_y = 30000,\n    nudge_x = 1000,\n    box.padding = 1.5,\n    point.padding = 0.5,\n    segment.size = 0.2,\n    segment.color = \"white\",\n    color = \"white\"\n  ) +\n  labs(\n    title = \"Gradient Descent (2D View)\",\n    y = \"intercept\",\n    color = \"cost (mse)\"\n  ) +\n  scale_color_distiller(palette = \"Blues\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe can see from these random samples of intercepts and slopes that the lowest cost is 754413237.731 with an intercept of 113438 and slope of 966. So no matter where we start (any intercept and slope combination), we should descend and ultimately end up with the lowest cost value of 754413237.731."
  },
  {
    "objectID": "posts/solving-ols-regression-using-gradient-descent/index.html#partial-derivatives",
    "href": "posts/solving-ols-regression-using-gradient-descent/index.html#partial-derivatives",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "4 Partial Derivatives",
    "text": "4 Partial Derivatives\nWhat are partial derivatives?\nPartial derivatives allow us to obtain a slope at any point that is specific to a variable on any function (or line). In our case, partial derivatives allow us to obtain a slope at any specific \\(\\beta\\) on the cost function of MSE, which can be denoted as \\(\\frac{\\partial MSE}{\\partial \\hat{\\beta}}\\).\n\nHow do we calculate the partial derivatives?\nFirst, we expand MSE.\n\\[\n\\tag{5}\n\\begin{align}\nMSE & = \\frac{(Y-X\\beta)^2}{N} \\\\\n& = \\frac{(Y-X\\beta)(Y-X\\beta)}{N} \\\\\n& = \\frac{(Y^2-2YX\\beta+X^2\\beta^2)}{N} \\\\\n& = \\frac{Y^2}{N} - \\frac{2YX\\beta}{N}+\\frac{X^2\\beta^2}{N} \\\\\n\\end{align}\n\\]\nTo calculate the partial derivative, we repeat the following for each term:\n\nSet the term without \\(\\beta\\) to 0\nMultiply the term by the exponent of \\(\\beta\\)\nSubtract 1 from the exponent of \\(\\beta\\)\n\nand simplify.\n\\[\n\\tag{6}\n\\begin{align}\n\\frac{\\partial MSE}{\\partial \\hat{\\beta}} & = 0 - \\frac{1*2YX\\beta^0}{N} + \\frac{2X^2\\beta^1}{N} \\\\\n& = \\frac{-2YX+2X^2\\beta}{N} \\\\\n& = \\frac{-2X(Y-X\\beta)}{N} \\\\\n& = \\frac{-2X\\epsilon}{N}\n\\end{align}\n\\] Note: We set the term without \\(\\beta\\) to 0 in step 1 because that term can be thought of as having \\(\\beta^0\\), which equals 1. When we then multiply the term by the exponent of 0, we end up with a term of 0."
  },
  {
    "objectID": "posts/solving-ols-regression-using-gradient-descent/index.html#update-coefficients",
    "href": "posts/solving-ols-regression-using-gradient-descent/index.html#update-coefficients",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "5 Update Coefficients",
    "text": "5 Update Coefficients\nWe can then use the information from partial derivatives to update the coefficients (\\(\\beta\\)) so that coefficients (\\(\\beta\\)) change and descend into a lower MSE.\n\nHow do we update the coefficients?\nWe can update the coefficients \\(\\beta\\) by subtracting the partial derivatives multiplied by the learning rate.\n\\[\n\\tag{7}\n\\hat{\\beta} = \\hat{\\beta} - \\frac{\\partial MSE}{\\partial \\hat{\\beta}}*learning\\ rate\n\\]\nA \\(\\beta\\) lower than it should be will have a negative slope and when we subtract the partial derivative (\\(\\frac{\\partial MSE}{\\partial \\hat{\\beta}}\\)) from the old \\(\\beta\\), the new \\(\\beta\\) will then be less negative. Conversely, a \\(\\beta\\) higher than should be will have a positive slope and when we subtract the partial derivative (\\(\\frac{\\partial MSE}{\\partial \\hat{\\beta}}\\)), the new \\(\\beta\\) will be less positive. When we are at the lowest cost value, the partial derivative (\\(\\frac{\\partial MSE}{\\partial \\hat{\\beta}}\\)) will be 0 and \\(\\beta\\) will stop updating.\n\nWhat is the learning rate?\nThe learning rate determines how fast the coefficients update (descends). A higher learning rate descends quickly but may be susceptible to skipping or moving past the global minima. A lower learning rate is more precise but is slower as it takes more time to compute.\n\n\nCode\nupdate_b &lt;- function(y, x, b, lr) {\n  y_pred &lt;- x %*% b\n  e &lt;- y - y_pred\n  derivatives &lt;- (-2 * t(x) %*% e ) / nrow(x)\n  b &lt;- b - derivatives * lr\n  return(b)\n}"
  },
  {
    "objectID": "posts/solving-ols-regression-using-gradient-descent/index.html#train",
    "href": "posts/solving-ols-regression-using-gradient-descent/index.html#train",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "6 Train",
    "text": "6 Train\nNow let’s train the data, arbitrarily starting the coefficients at 0 and a learning rate of 0.0001 for 50,000 iterations.\n\n\nCode\n# set number of iterations\niter &lt;- 50000\n\n# set learning rate\nlr &lt;- 0.0001\n\n# set initial values of coefficients\nb &lt;- NULL\nfor (i in 1:ncol(x)) {\n  b[i] &lt;- 0\n}\nb &lt;- as.matrix(b)\n\ncat(paste(\"iteration\", \"intercept\", \"slope\", \"cost\\n\", \n          \"0\", round(b[1], n_round), round(b[2], n_round), round(obtain_cost(y, x, b), n_round)))\n\n\niteration intercept slope cost\n 0 0 0 13844273659.244\n\n\nCode\n# set initial training dataset\ndf_train &lt;- tibble(\n  iter = NA,\n  int = NA,\n  slope = NA,\n  cost = NA,\n  .rows = iter\n)\n\n# train and save training history using for loop\nfor (i in 1:iter) {\n  b &lt;- update_b(y, x, b, lr)\n  df_train$iter[i] &lt;- i\n  df_train$int[i] &lt;- b[1]\n  df_train$slope[i] &lt;- b[2]\n  df_train$cost[i] &lt;- obtain_cost(y, x, b)\n  if (i %in% round(seq(1, iter, length.out = 10), 0)) {\n    cat(paste(\"\\n\", round(i, n_round), round(b[1], n_round), round(b[2], n_round), round(obtain_cost(y, x, b), n_round)))\n  }\n}\n\n\n\n 1 22.741 32.646 13828621661.101\n 5556 76282.576 985.342 2154826164.687\n 11112 101389.243 985.342 905992994.67\n 16667 109651.717 985.342 770720117.435\n 22223 112371.933 985.342 756060150.645\n 27778 113267.142 985.342 754472191.632\n 33334 113561.867 985.342 754300099.289\n 38889 113658.86 985.342 754281458.348\n 44445 113690.793 985.342 754279438.168\n 50000 113701.301 985.342 754279219.343\n\n\n\n\n6.1 Plot Training\n\n\nCode\nggplot(df_train, aes(iter, cost)) +\n  geom_line() +\n  theme_minimal() +\n  labs(\n    x = \"iterations\",\n    y = \"cost (mse)\"\n  )\n\n\n\n\n\n\n\n\n\nWe can see that cost (MSE) flattens out around 15,000 iterations.\n\n\n\n6.2 Plot Gradient Descent Path\nLet’s visualize the path that the gradient descent took, starting from our initial intercept and slope value of 0.\n\n\nCode\nn_sample &lt;- 100000\ndf_gd &lt;- tibble(\n  int = sample(seq(0, 150000), n_sample, T),\n  slope = sample(seq(0, 2000), n_sample, T)\n) %&gt;%\n  rowwise() %&gt;%\n  mutate(cost = obtain_cost(y, x, b = c(int, slope)))\n\ndf_train_sorted &lt;- df_train %&gt;% arrange(cost, iter)\ndf_min &lt;- df_train_sorted[1, ]\n\nggplot(df_gd, aes(slope, int, color = cost)) +\n  geom_point() +\n  scale_color_distiller(palette = \"Blues\") +\n  theme_minimal() +\n  labs(\n    y = \"intercept\",\n    color = \"cost (mse)\"\n  ) +\n  geom_line(data = df_train, mapping = aes(slope, int), color = \"black\", size = 0.5, alpha = 0.5) +\n  geom_point(data = df_min, mapping = aes(slope, int), color = \"white\", shape = 21, alpha = .5) +\n  geom_text_repel(\n    data = df_min,\n    mapping = aes(label = paste0(\"min(cost) = \", round(cost, n_round), \"\\nintercept = \", round(int, n_round), \"\\nslope = \", round(slope, n_round))),\n    nudge_y = 30000,\n    nudge_x = 1000,\n    box.padding = 1.5,\n    point.padding = 0.5,\n    segment.size = 0.2,\n    segment.color = \"white\",\n    color = \"white\"\n  )\n\n\n\n\n\n\n\n\n\nWe can see that most of the iterations were spent optimizing the slope with minimal changes to the intercept as well as the cost (MSE) up until around iteration 1000. Then, the optimal intercept was found within a few iterations, which dramatically reduced cost (MSE)."
  },
  {
    "objectID": "posts/solving-ols-regression-using-gradient-descent/index.html#compare-gradient-descent-and-lm",
    "href": "posts/solving-ols-regression-using-gradient-descent/index.html#compare-gradient-descent-and-lm",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "7 Compare Gradient Descent and lm()",
    "text": "7 Compare Gradient Descent and lm()\nWe can compare the results of gradient descent with lm() to ensure that we did this correctly. We should approximately have the same results within rounding error because OLS is a convex shape with only one global minima.\n\n\n7.1 Solve OLS using Gradient Descent\n\n\nCode\ncoef &lt;- c(\"(Intercept)\" = df_min$int, \"yrs.since.phd\" = df_min$slope)\nrmse_coef &lt;- (solve(t(x) %*% x) * df_min$cost) %&gt;%\n  diag() %&gt;%\n  sqrt()\nt_stat &lt;- coef / rmse_coef\nn &lt;- nrow(x)\np &lt;- ncol(x)\ndf &lt;- n - p\np_value &lt;- 2 * pt(t_stat, df, lower = FALSE)\ntibble(\n  term = colnames(x),\n  estimate = coef,\n  std.error = rmse_coef,\n  statistic = t_stat,\n  p.value = p_value\n)\n\n\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    113701.     1378.     82.5  4.24e-251\n2 yrs.since.phd     985.      107.      9.20 2.09e- 18\n\n\n\n\n\n7.2 Solve OLS using lm()\n\n\nCode\nmodel &lt;- lm(y ~ 0 + x)\ntidy(model)\n\n\n# A tibble: 2 × 5\n  term           estimate std.error statistic   p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 x(Intercept)    113706.     1382.     82.3  1.07e-250\n2 xyrs.since.phd     985.      107.      9.18 2.50e- 18\n\n\n\n\n\n7.3 Cost\n\n\nCode\nglue(\"Cost using GD: {round(df_min$cost, n_round)}\")\n\n\nCost using GD: 754279219.343\n\n\nCode\nglue(\"Cost using lm(): {round(glance(model)$sigma^2, n_round)}\")\n\n\nCost using lm(): 758098327.9\n\n\nWe can see that solving OLS regression using gradient descent produced a lower cost function (MSE) than by using lm(). The lower MSE using gradient descent was produced from using a smaller intercept. Since MSE was smaller using gradient descent, the standard error was also smaller using gradient descent and subsequently a larger statistic and lower p-value.\n\n\n\n7.4 Figure\n\n\nCode\nfig_gd &lt;- ggplot(mapping = aes(x[, 2], y)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = coef[1], slope = coef[2], size = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Solved using Gradient Descent\",\n    x = \"yrs.since.phd\",\n    y = \"salary\"\n  )\n\nfig_lm &lt;- ggplot(mapping = aes(x[, 2], y)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(color = \"black\", method = \"lm\", se = F) +\n  theme_minimal() +\n  labs(\n    title = \"Solved using lm()\",\n    x = \"yrs.since.phd\",\n    y = \"salary\"\n  )\n\ngridExtra::grid.arrange(fig_gd, fig_lm, nrow = 1)\n\n\n\n\n\n\n\n\n\nHowever, since the range of salary is so large, each method produced nearly identical graphs."
  },
  {
    "objectID": "posts/solving-ols-regression-using-gradient-descent/index.html#acknowledgements",
    "href": "posts/solving-ols-regression-using-gradient-descent/index.html#acknowledgements",
    "title": "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent",
    "section": "8 Acknowledgements",
    "text": "8 Acknowledgements\n\n@bfortuner for his gradient descent in python tutorial.\n@mychan24 for her helpful feedback and suggestions to this blog.\n@praxling for her proofing and feedback to this blog.\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "posts/simple-linear-regression-via-voice-command-a-shiny-app/index.html",
    "href": "posts/simple-linear-regression-via-voice-command-a-shiny-app/index.html",
    "title": "Simple Linear Regression via Voice Command: A Shiny App",
    "section": "",
    "text": "If you are not redirected, please click here."
  },
  {
    "objectID": "posts/exploring-11-years-of-chicago-blackhawks-data-using-principal-components-analysis/index.html",
    "href": "posts/exploring-11-years-of-chicago-blackhawks-data-using-principal-components-analysis/index.html",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "",
    "text": "If you are not redirected, please click here."
  }
]