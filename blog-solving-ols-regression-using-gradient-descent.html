<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Ekarin Eric Pongpipat, M.A." />


<title>Solving Ordinary Least Squares (OLS) Regression using Gradient Descent</title>

<script src="site_libs/header-attrs-2.3/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/lumen.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script src="site_libs/navigation-1.1/sourceembed.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116039246-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116039246-1');
</script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
#rmd-source-code {
  display: none;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 54px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h2 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h3 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h4 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h5 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h6 {
  padding-top: 59px;
  margin-top: -59px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">



<nav class="navbar navbar-default navbar-fixed-top">
  <div class="container">

    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      <a class="navbar-brand" href="index.html"><span class="fas fa-terminal" aria-hidden="true"></span></a>
      <a class="navbar-brand" href="index.html"><img src="images/icons8-artificial-intelligence-100.png" style="height: 20px"></a>
      <a class="navbar-brand" href="index.html"><img src="images/icons8-scatter-plot-96.png" style="height: 20px"></a>
      <a class="navbar-brand" href="index.html">Ekarin Eric Pongpipat, M.A.</a>
    </div> <!-- /.navbar-header -->

    <div class="collapse navbar-collapse" id="navbar-collapse">
      <ul class="nav navbar-nav navbar-right">
        <li><a href="blog.html"><span class="fas fa-quote-left" aria-hidden="true"></span> &ensp;Blog</a></li>
         <li><a href="books.html"><span class="fas fa-book" aria-hidden="true"></span> &ensp;eBooks</a></li>
        <li><a href="cv.html"><span class="far fa-id-badge" aria-hidden="true"></span> &ensp;CV</a></li>
        <li><a href="https://github.com/epongpipat" target="_blank"><span class="fa fa-github" aria-hidden="true"></span> &ensp;GitHub</a></li>

        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"><span class="fas fa-handshake" aria-hidden="true"></span> &ensp;Connect<span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="mailto:epongpipat@gmail.com" target="_blank"><span class="fa fa-envelope" aria-hidden="true"></span> &ensp;Email</a></li>
            <li role="separator" class="divider"></li>
            <li><a href="https://twitter.com/epongpipat" target="_blank"><span class="fab fa-twitter" aria-hidden="true"></span> &ensp;Twitter</a></li>
            <li><a href="https://www.researchgate.net/profile/Ekarin_Pongpipat" target="_blank"><span class="fab fa-researchgate" aria-hidden="true"></span> &ensp;ResearchGate</a></li>
            <li><a href="https://www.linkedin.com/in/epongpipat/" target="_blank"><span class="fa fa-linkedin" aria-hidden="true"></span> &ensp;LinkedIn</a></li>
          </ul>
        </li>

      </ul>
    </div> <!-- /.navbar-collapse -->

  </div> <!-- /.container -->
</nav> <!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
<li role="separator" class="divider"></li>
<li><a id="rmd-download-source" href="#">Download Rmd</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Solving Ordinary Least Squares (OLS) Regression using Gradient Descent</h1>
<h4 class="author">Ekarin Eric Pongpipat, M.A.</h4>

</div>


<p><font color="black">Original Post: 6/10/2019 <br>Modified Post: 6/17/2019</font></p>
<p><br></p>
<p>Already familiar with OLS regression and wanted to learn about gradient descent? This blog post will provide a brief tutorial on solving OLS using gradient descent using R.</p>
<p><br></p>
<p>Remember that the equation for OLS is:</p>
<p><span class="math display">\[
\tag{1}
Y = X\beta + \epsilon
\]</span></p>
<p>where <span class="math inline">\(Y\)</span> is a column-wise vector of DVs, <span class="math inline">\(X\)</span> is a matrix of IVs, <span class="math inline">\(\beta\)</span> is a column-wise vector of regression coefficients, and <span class="math inline">\(\epsilon\)</span> is a column-wise vector of error.</p>
<p><span class="math display">\[
\begin{bmatrix}
\tag{2}
y_1 \\
y_2 \\
. \\
. \\
. \\
y_n \\
\end{bmatrix} = 
\begin{bmatrix}
1 &amp; x_{1, 1} &amp; x_{1, 2} &amp; . &amp; . &amp; . &amp; x_{1, n} \\
1 &amp; x_{2, 1} &amp; x_{2, 2} &amp; . &amp; . &amp; . &amp; x_{2, n}\\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; .\\
1 &amp; x_{n, 1} &amp; x_{n, 2} &amp; . &amp; . &amp; . &amp; x_{n, n}\\
\end{bmatrix}
\begin{bmatrix}
b_0 \\
b_1 \\
. \\
. \\
. \\
b_n \\
\end{bmatrix} +
\begin{bmatrix}
e_1 \\
e_2 \\
. \\
. \\
. \\
e_n \\
\end{bmatrix}
\]</span></p>
<p>For a review of matrix multiplication, check out my <a href="blog_solving_ols_regression_using_matrix_algebra.html">previous blog post</a>.</p>
<p><br></p>
<p>OLS tries to minimize error using the mean squared error (MSE) formula:</p>
<p><span class="math display">\[
\tag{3}
MSE = \frac{\Sigma (Y-\hat{Y})^2}{N} = \frac{\Sigma e_i^2}{N} = mean(e_i^2)
\]</span></p>
<center>
where <span class="math inline">\(\hat{Y} = X\beta\)</span>
</center>
<p><br></p>
<p>Note: MSE is a single cost function (or formula) from many that we could choose from. For example, another cost function is mean absolute error (MAE):</p>
<p><span class="math display">\[
\tag{4}
MAE = \frac{\Sigma |Y-\hat{Y}|}{N} = \frac{\Sigma|e_i|}{N} = mean(abs(e_i))
\]</span></p>
<pre class="r"><code>obtain_cost &lt;- function(y, x, b) {
  y_pred &lt;- x %*% b
  e &lt;- y - y_pred
  se &lt;- e^2
  mse &lt;- mean(se)
  return(mse)
}</code></pre>
<p><br></p>
<blockquote>
<p>Let’s say that we are interested in understanding if the number of years a professor has had their Ph.D. is associated with a higher 9-month academic salary.</p>
</blockquote>
<p><br></p>
<div id="r-libraries" class="section level2">
<h2>R Libraries</h2>
<p>First, let’s load some packages.</p>
<pre class="r"><code>packages &lt;- c(&quot;tidyverse&quot;,   # data manipulation and visualization
              &quot;carData&quot;,     # data set to obtain professor&#39;s 9-month salary
              &quot;broom&quot;,       # nice table format of cofficients from lm()
              &quot;ggrepel&quot;,     # ggplot extension to repel text and extra features
              &quot;glue&quot;,        # concatentate strings and alias
              &quot;plot3D&quot;,      # plot 3D figures
              &quot;RColorBrewer&quot; # load color palettes
              )
xfun::pkg_attach2(packages, message = F)
n_round &lt;- 3</code></pre>
<p><br></p>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<p>Then, let’s define our independent variable (IV: <code>yrs.since.phd</code>) as <code>x</code> and our dependent variable (DV: <code>salary</code>) as <code>y</code> using the <code>Salaries</code> dataset in the <code>carData</code> package.</p>
<pre class="r"><code>x &lt;- model.matrix(~ scale(yrs.since.phd, scale = F), Salaries)
colnames(x) &lt;- c(&quot;(Intercept)&quot;, &quot;yrs.since.phd&quot;)
dim(x)</code></pre>
<pre><code>## [1] 397   2</code></pre>
<pre class="r"><code>head(x)</code></pre>
<pre><code>##   (Intercept) yrs.since.phd
## 1           1         -3.31
## 2           1         -2.31
## 3           1        -18.31
## 4           1         22.69
## 5           1         17.69
## 6           1        -16.31</code></pre>
<pre class="r"><code>y &lt;- Salaries$salary %&gt;% as.matrix()
colnames(y) &lt;- c(&quot;salary&quot;)
dim(y)</code></pre>
<pre><code>## [1] 397   1</code></pre>
<pre class="r"><code>head(y)</code></pre>
<pre><code>##      salary
## [1,] 139750
## [2,] 173200
## [3,]  79750
## [4,] 115000
## [5,] 141500
## [6,]  97000</code></pre>
<p><br></p>
</div>
<div id="gradient-descent" class="section level2">
<h2>Gradient Descent</h2>
<p>What is gradient descent?</p>
<p>Starting at any position (e.g., intercept and slope combination), gradient descent takes the partial derivative of each coefficient (<span class="math inline">\(\beta\)</span>) from the cost function (MSE) and moves (or descends) in the direction that will continue to minimize the cost (or gradient) function.</p>
<pre class="r"><code># random sample of possible intercepts and slopes
# then calculate cost function (mse) for each intercept and slope combination
n_sample &lt;- 200000
df_gd &lt;- tibble(
  int = sample(seq(0, 200000), n_sample, T),
  slope = sample(seq(-10000, 10000), n_sample, T)
) %&gt;%
  rowwise() %&gt;%
  mutate(cost = obtain_cost(y, x, b = c(int, slope)))</code></pre>
<pre class="r"><code>scatter3D(x = df_gd$int,
          y = df_gd$slope,
          z = df_gd$cost,
          xlab = &quot;intercept&quot;,
          ylab = &quot;slope&quot;,
          zlab = &quot;cost (mse)&quot;,
          col = ramp.col(col = sort(RColorBrewer::brewer.pal(9, &quot;Blues&quot;), decreasing = F),
                         n = length(unique(df_gd$cost))),
          colkey = F,
          phi = 10,
          theta = 45,
          main = &quot;Gradient Descent (3D View)&quot;
          )</code></pre>
<p><img src="blog-solving-ols-regression-using-gradient-descent_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code># plot intercept and slope, and color by cost (mse)
# highlight and label min(cost)
ggplot(df_gd, aes(slope, int, color = cost)) +
  geom_point() +
  geom_point(data = subset(df_gd, cost == min(cost)), color = &quot;black&quot;, shape = 21, alpha = .5) +
  geom_text_repel(
    data = subset(df_gd, cost == min(cost)),
    mapping = aes(label = paste0(&quot;min(cost) = &quot;, round(cost, n_round), &quot;\nintercept = &quot;, round(int, n_round), &quot;\nslope = &quot;, round(slope, n_round))),
    nudge_y = 30000,
    nudge_x = 1000,
    box.padding = 1.5,
    point.padding = 0.5,
    segment.size = 0.2,
    segment.color = &quot;black&quot;,
    color = &quot;black&quot;
  ) +
  labs(
    title = &quot;Gradient Descent (2D View)&quot;,
    y = &quot;intercept&quot;,
    color = &quot;cost (mse)&quot;
  ) +
  scale_color_distiller(palette = &quot;Blues&quot;) +
  theme_minimal()</code></pre>
<p><img src="blog-solving-ols-regression-using-gradient-descent_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>We can see from these random samples of intercepts and slopes that the lowest cost is 754432699.194 with an intercept of 114009 and slope of 966. So no matter where we start (any intercept and slope combination), we should descend and ultimately end up with the lowest cost value of 754432699.194.</p>
<p><br></p>
</div>
<div id="partial-derivatives" class="section level2">
<h2>Partial Derivatives</h2>
<p>What are partial derivatives?</p>
<p>Partial derivatives allow us to obtain a slope at any point that is specific to a variable on any function (or line). In our case, partial derivatives allow us to obtain a slope at any specific <span class="math inline">\(\beta\)</span> on the cost function of MSE, which can be denoted as <span class="math inline">\(\frac{\partial MSE}{\partial \hat{\beta}}\)</span>.</p>
<p><br></p>
<p>How do we calculate the partial derivatives?</p>
<p>First, we expand MSE.</p>
<p><span class="math display">\[ 
\tag{5}
\begin{align}
MSE &amp; = \frac{(Y-X\beta)^2}{N} \\
&amp; = \frac{(Y-X\beta)(Y-X\beta)}{N} \\
&amp; = \frac{(Y^2-2YX\beta+X^2\beta^2)}{N} \\
&amp; = \frac{Y^2}{N} - \frac{2YX\beta}{N}+\frac{X^2\beta^2}{N} \\
\end{align}
\]</span></p>
<p>To calculate the partial derivative, we repeat the following for each term:</p>
<ol style="list-style-type: decimal">
<li>Set the term without <span class="math inline">\(\beta\)</span> to 0</li>
<li>Multiply the term by the exponent of <span class="math inline">\(\beta\)</span></li>
<li>Subtract 1 from the exponent of <span class="math inline">\(\beta\)</span></li>
</ol>
<p>and simplify.</p>
<p><span class="math display">\[
\tag{6}
\begin{align}
\frac{\partial MSE}{\partial \hat{\beta}} &amp; = 0 - \frac{1*2YX\beta^0}{N} + \frac{2X^2\beta^1}{N} \\
&amp; = \frac{-2YX+2X^2\beta}{N} \\
&amp; = \frac{-2X(Y-X\beta)}{N} \\
&amp; = \frac{-2X\epsilon}{N}
\end{align}
\]</span> Note: We set the term without <span class="math inline">\(\beta\)</span> to 0 in step 1 because that term can be thought of as having <span class="math inline">\(\beta^0\)</span>, which equals 1. When we then multiply the term by the exponent of 0, we end up with a term of 0.</p>
<p><br></p>
</div>
<div id="update-coefficients" class="section level2">
<h2>Update Coefficients</h2>
<p>We can then use the information from partial derivatives to update the coefficients (<span class="math inline">\(\beta\)</span>) so that coefficients (<span class="math inline">\(\beta\)</span>) change and descend into a lower MSE.</p>
<p><br></p>
<p>How do we update the coefficients?</p>
<p>We can update the coefficients <span class="math inline">\(\beta\)</span> by subtracting the partial derivatives multiplied by the learning rate.</p>
<p><span class="math display">\[
\tag{7}
\hat{\beta} = \hat{\beta} - \frac{\partial MSE}{\partial \hat{\beta}}*learning\ rate
\]</span></p>
<p>A <span class="math inline">\(\beta\)</span> lower than it should be will have a negative slope and when we subtract the partial derivative (<span class="math inline">\(\frac{\partial MSE}{\partial \hat{\beta}}\)</span>) from the old <span class="math inline">\(\beta\)</span>, the new <span class="math inline">\(\beta\)</span> will then be less negative. Conversely, a <span class="math inline">\(\beta\)</span> higher than should be will have a positive slope and when we subtract the partial derivative (<span class="math inline">\(\frac{\partial MSE}{\partial \hat{\beta}}\)</span>), the new <span class="math inline">\(\beta\)</span> will be less positive. When we are at the lowest cost value, the partial derivative (<span class="math inline">\(\frac{\partial MSE}{\partial \hat{\beta}}\)</span>) will be 0 and <span class="math inline">\(\beta\)</span> will stop updating.</p>
<p><br></p>
<p>What is the learning rate?</p>
<p>The learning rate determines how fast the coefficients update (descends). A higher learning rate descends quickly but may be susceptible to skipping or moving past the global minima. A lower learning rate is more precise but is slower as it takes more time to compute.</p>
<pre class="r"><code>update_b &lt;- function(y, x, b, lr) {
  y_pred &lt;- x %*% b
  e &lt;- y - y_pred
  derivatives &lt;- (-2 * t(x) %*% e ) / nrow(x)
  b &lt;- b - derivatives * lr
  return(b)
}</code></pre>
<p><br></p>
</div>
<div id="train" class="section level2">
<h2>Train</h2>
<p>Now let’s train the data, arbitrarily starting the coefficients at 0 and a learning rate of 0.0001 for 50,000 iterations.</p>
<pre class="r"><code># set number of iterations
iter &lt;- 50000

# set learning rate
lr &lt;- 0.0001

# set initial values of coefficients
b &lt;- NULL
for (i in 1:ncol(x)) {
  b[i] &lt;- 0
}
b &lt;- as.matrix(b)

cat(paste(&quot;iteration&quot;, &quot;intercept&quot;, &quot;slope&quot;, &quot;cost\n&quot;, 
          &quot;0&quot;, round(b[1], n_round), round(b[2], n_round), round(obtain_cost(y, x, b), n_round)))</code></pre>
<pre><code>## iteration intercept slope cost
##  0 0 0 13844273659.244</code></pre>
<pre class="r"><code># set initial training dataset
df_train &lt;- tibble(
  iter = NA,
  int = NA,
  slope = NA,
  cost = NA,
  .rows = iter
)

# train and save training history using for loop
for (i in 1:iter) {
  b &lt;- update_b(y, x, b, lr)
  df_train$iter[i] &lt;- i
  df_train$int[i] &lt;- b[1]
  df_train$slope[i] &lt;- b[2]
  df_train$cost[i] &lt;- obtain_cost(y, x, b)
  if (i %in% round(seq(1, iter, length.out = 10), 0)) {
    cat(paste(&quot;\n&quot;, round(i, n_round), round(b[1], n_round), round(b[2], n_round), round(obtain_cost(y, x, b), n_round)))
  }
}</code></pre>
<pre><code>## 
##  1 22.741 32.646 13828621661.101
##  5556 76282.576 985.342 2154826164.687
##  11112 101389.243 985.342 905992994.67
##  16667 109651.717 985.342 770720117.435
##  22223 112371.933 985.342 756060150.645
##  27778 113267.142 985.342 754472191.632
##  33334 113561.867 985.342 754300099.289
##  38889 113658.86 985.342 754281458.348
##  44445 113690.793 985.342 754279438.168
##  50000 113701.301 985.342 754279219.343</code></pre>
<p><br></p>
<div id="plot-training" class="section level3">
<h3>Plot Training</h3>
<pre class="r"><code>ggplot(df_train, aes(iter, cost)) +
  geom_line() +
  theme_minimal() +
  labs(
    x = &quot;iterations&quot;,
    y = &quot;cost (mse)&quot;
  )</code></pre>
<p><img src="blog-solving-ols-regression-using-gradient-descent_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>We can see that cost (MSE) flattens out around 15,000 iterations.</p>
<p><br></p>
</div>
<div id="plot-gradient-descent-path" class="section level3">
<h3>Plot Gradient Descent Path</h3>
<p>Let’s visualize the path that the gradient descent took, starting from our initial intercept and slope value of 0.</p>
<pre class="r"><code>n_sample &lt;- 100000
df_gd &lt;- tibble(
  int = sample(seq(0, 150000), n_sample, T),
  slope = sample(seq(0, 2000), n_sample, T)
) %&gt;%
  rowwise() %&gt;%
  mutate(cost = obtain_cost(y, x, b = c(int, slope)))

df_train_sorted &lt;- df_train %&gt;% arrange(cost, iter)
df_min &lt;- df_train_sorted[1, ]

ggplot(df_gd, aes(slope, int, color = cost)) +
  geom_point() +
  scale_color_distiller(palette = &quot;Blues&quot;) +
  theme_minimal() +
  labs(
    y = &quot;intercept&quot;,
    color = &quot;cost (mse)&quot;
  ) +
  geom_line(data = df_train, mapping = aes(slope, int), color = &quot;black&quot;, size = 0.5, alpha = 0.5) +
  geom_point(data = df_min, mapping = aes(slope, int), color = &quot;black&quot;, shape = 21, alpha = .5) +
  geom_text_repel(
    data = df_min,
    mapping = aes(label = paste0(&quot;min(cost) = &quot;, round(cost, n_round), &quot;\nintercept = &quot;, round(int, n_round), &quot;\nslope = &quot;, round(slope, n_round))),
    nudge_y = 30000,
    nudge_x = 1000,
    box.padding = 1.5,
    point.padding = 0.5,
    segment.size = 0.2,
    segment.color = &quot;black&quot;,
    color = &quot;black&quot;
  )</code></pre>
<p><img src="blog-solving-ols-regression-using-gradient-descent_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>We can see that most of the iterations were spent optimizing the slope with minimal changes to the intercept as well as the cost (MSE) up until around iteration 1000. Then, the optimal intercept was found within a few iterations, which dramatically reduced cost (MSE).</p>
<p><br></p>
</div>
</div>
<div id="compare-gradient-descent-and-lm" class="section level2">
<h2>Compare Gradient Descent and <code>lm()</code></h2>
<p>We can compare the results of gradient descent with <code>lm()</code> to ensure that we did this correctly. We should approximately have the same results within rounding error because OLS is a convex shape with only one global minima.</p>
<p><br></p>
<div id="solve-ols-using-gradient-descent" class="section level3">
<h3>Solve OLS using Gradient Descent</h3>
<pre class="r"><code>coef &lt;- c(&quot;(Intercept)&quot; = df_min$int, &quot;yrs.since.phd&quot; = df_min$slope)
rmse_coef &lt;- (solve(t(x) %*% x) * df_min$cost) %&gt;%
  diag() %&gt;%
  sqrt()
t_stat &lt;- coef / rmse_coef
n &lt;- nrow(x)
p &lt;- ncol(x)
df &lt;- n - p
p_value &lt;- 2 * pt(t_stat, df, lower = FALSE)
tibble(
  term = colnames(x),
  estimate = coef,
  std.error = rmse_coef,
  statistic = t_stat,
  p.value = p_value
)</code></pre>
<pre><code>## # A tibble: 2 x 5
##   term          estimate std.error statistic   p.value
##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)    113701.     1378.     82.5  4.24e-251
## 2 yrs.since.phd     985.      107.      9.20 2.09e- 18</code></pre>
<p><br></p>
</div>
<div id="solve-ols-using-lm" class="section level3">
<h3>Solve OLS using <code>lm()</code></h3>
<pre class="r"><code>model &lt;- lm(y ~ x)
tidy(model)</code></pre>
<pre><code>## # A tibble: 3 x 5
##   term           estimate std.error statistic    p.value
##   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept)     113706.     1382.     82.3   1.07e-250
## 2 x(Intercept)        NA        NA      NA    NA        
## 3 xyrs.since.phd     985.      107.      9.18  2.50e- 18</code></pre>
<p><br></p>
</div>
<div id="cost" class="section level3">
<h3>Cost</h3>
<pre class="r"><code>glue(&quot;Cost using GD: {round(df_min$cost, n_round)}\nCost using lm(): {round(glance(model)$sigma^2, n_round)}&quot;)</code></pre>
<pre><code>## Cost using GD: 754279219.343
## Cost using lm(): 758098327.9</code></pre>
<p>We can see that solving OLS regression using gradient descent produced a lower cost function (MSE) than by using <code>lm()</code>. The lower MSE using gradient descent was produced from using a smaller intercept. Since MSE was smaller using gradient descent, the standard error was also smaller using gradient descent and subsequently a larger statistic and lower p-value.</p>
<p><br></p>
</div>
<div id="figure" class="section level3">
<h3>Figure</h3>
<pre class="r"><code>fig_gd &lt;- ggplot(mapping = aes(x[, 2], y)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = coef[1], slope = coef[2], size = 1) +
  theme_minimal() +
  labs(
    title = &quot;Solved using Gradient Descent&quot;,
    x = &quot;yrs.since.phd&quot;,
    y = &quot;salary&quot;
  )

fig_lm &lt;- ggplot(mapping = aes(x[, 2], y)) +
  geom_point(alpha = 0.5) +
  geom_smooth(color = &quot;black&quot;, method = &quot;lm&quot;, se = F) +
  theme_minimal() +
  labs(
    title = &quot;Solved using lm()&quot;,
    x = &quot;yrs.since.phd&quot;,
    y = &quot;salary&quot;
  )

gridExtra::grid.arrange(fig_gd, fig_lm, nrow = 1)</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="blog-solving-ols-regression-using-gradient-descent_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>However, since the range of salary is so large, each method produced nearly identical graphs.</p>
<p><br></p>
</div>
</div>
<div id="acknowledgements" class="section level2">
<h2>Acknowledgements</h2>
<p><a href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html" target="_blank"><span class="citation">@bfortuner</span></a> for his gradient descent in python tutorial.</p>
<p><a href='https://github.com/mychan24' target='_blank'><span class="citation">@mychan24</span></a> for her helpful feedback and suggestions to this blog.</p>
<p><a href='https://github.com/praxling' target='_blank'><span class="citation">@praxling</span></a> for her proofing and feedback to this blog.</p>
<p><br></p>
<hr>
<p><br></p>
<!-- disqus START -->
<div id="disqus_thread">

</div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = 'https://ekarinpongpipat.com/blog_solving_ols_regression_using_gradient_descent.html';  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = 'blog_solving_ols_regression_using_gradient_descent'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://epongpipat.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>
Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>
<!-- disqus END -->
</div>

<div id="rmd-source-code">---
title: "Solving Ordinary Least Squares (OLS) Regression using Gradient Descent"
author: "Ekarin Eric Pongpipat, M.A."
output: 
  html_document:
    highlight: textmate
    theme: lumen
    code_download: TRUE
    code_folding: show
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---

<font color="black">Original Post: 6/10/2019
<br>Modified Post: 6/17/2019</font>

```{r, include=F}
options(scipen = 999, digits = 3)
```

<br>

Already familiar with OLS regression and wanted to learn about gradient descent? This blog post will provide a brief tutorial on solving OLS using gradient descent using R.

<br>

Remember that the equation for OLS is:

$$
\tag{1}
Y = X\beta + \epsilon
$$

where $Y$ is a column-wise vector of DVs, $X$ is a matrix of IVs, $\beta$ is a column-wise vector of regression coefficients, and $\epsilon$ is a column-wise vector of error.

$$
\begin{bmatrix}
\tag{2}
y_1 \\
y_2 \\
. \\
. \\
. \\
y_n \\
\end{bmatrix} = 
\begin{bmatrix}
1 & x_{1, 1} & x_{1, 2} & . & . & . & x_{1, n} \\
1 & x_{2, 1} & x_{2, 2} & . & . & . & x_{2, n}\\
. & . & . & . & . & . & .\\
. & . & . & . & . & . & .\\
. & . & . & . & . & . & .\\
1 & x_{n, 1} & x_{n, 2} & . & . & . & x_{n, n}\\
\end{bmatrix}
\begin{bmatrix}
b_0 \\
b_1 \\
. \\
. \\
. \\
b_n \\
\end{bmatrix} +
\begin{bmatrix}
e_1 \\
e_2 \\
. \\
. \\
. \\
e_n \\
\end{bmatrix}
$$

For a review of matrix multiplication, check out my [previous blog post](blog_solving_ols_regression_using_matrix_algebra.html).

<br>

OLS tries to minimize error using the mean squared error (MSE) formula:

$$
\tag{3}
MSE = \frac{\Sigma (Y-\hat{Y})^2}{N} = \frac{\Sigma e_i^2}{N} = mean(e_i^2)
$$

<center>where $\hat{Y} = X\beta$</center>

<br>

Note: MSE is a single cost function (or formula) from many that we could choose from. For example, another cost function is mean absolute error (MAE): 

$$
\tag{4}
MAE = \frac{\Sigma |Y-\hat{Y}|}{N} = \frac{\Sigma|e_i|}{N} = mean(abs(e_i))
$$


```{r}
obtain_cost <- function(y, x, b) {
  y_pred <- x %*% b
  e <- y - y_pred
  se <- e^2
  mse <- mean(se)
  return(mse)
}
```

<br>

> Let's say that we are interested in understanding if the number of years a professor has had their Ph.D. is associated with a higher 9-month academic salary. 

<br>

## R Libraries

First, let's load some packages. 

```{r}
packages <- c("tidyverse",   # data manipulation and visualization
              "carData",     # data set to obtain professor's 9-month salary
              "broom",       # nice table format of cofficients from lm()
              "ggrepel",     # ggplot extension to repel text and extra features
              "glue",        # concatentate strings and alias
              "plot3D",      # plot 3D figures
              "RColorBrewer" # load color palettes
              )
xfun::pkg_attach2(packages, message = F)
n_round <- 3
```

<br>

## Data

Then, let's define our independent variable (IV: `yrs.since.phd`) as `x` and our dependent variable  (DV: `salary`) as `y` using the `Salaries` dataset in the `carData` package.
```{r}
x <- model.matrix(~ scale(yrs.since.phd, scale = F), Salaries)
colnames(x) <- c("(Intercept)", "yrs.since.phd")
dim(x)
head(x)
y <- Salaries$salary %>% as.matrix()
colnames(y) <- c("salary")
dim(y)
head(y)
```

<br>

## Gradient Descent

What is gradient descent?

Starting at any position (e.g., intercept and slope combination), gradient descent takes the partial derivative of each coefficient ($\beta$) from the cost function (MSE) and moves (or descends) in the direction that will continue to minimize the cost (or gradient) function.

```{r}
# random sample of possible intercepts and slopes
# then calculate cost function (mse) for each intercept and slope combination
n_sample <- 200000
df_gd <- tibble(
  int = sample(seq(0, 200000), n_sample, T),
  slope = sample(seq(-10000, 10000), n_sample, T)
) %>%
  rowwise() %>%
  mutate(cost = obtain_cost(y, x, b = c(int, slope)))
```

```{r}
scatter3D(x = df_gd$int,
          y = df_gd$slope,
          z = df_gd$cost,
          xlab = "intercept",
          ylab = "slope",
          zlab = "cost (mse)",
          col = ramp.col(col = sort(RColorBrewer::brewer.pal(9, "Blues"), decreasing = F),
                         n = length(unique(df_gd$cost))),
          colkey = F,
          phi = 10,
          theta = 45,
          main = "Gradient Descent (3D View)"
          )
```

```{r}
# plot intercept and slope, and color by cost (mse)
# highlight and label min(cost)
ggplot(df_gd, aes(slope, int, color = cost)) +
  geom_point() +
  geom_point(data = subset(df_gd, cost == min(cost)), color = "black", shape = 21, alpha = .5) +
  geom_text_repel(
    data = subset(df_gd, cost == min(cost)),
    mapping = aes(label = paste0("min(cost) = ", round(cost, n_round), "\nintercept = ", round(int, n_round), "\nslope = ", round(slope, n_round))),
    nudge_y = 30000,
    nudge_x = 1000,
    box.padding = 1.5,
    point.padding = 0.5,
    segment.size = 0.2,
    segment.color = "black",
    color = "black"
  ) +
  labs(
    title = "Gradient Descent (2D View)",
    y = "intercept",
    color = "cost (mse)"
  ) +
  scale_color_distiller(palette = "Blues") +
  theme_minimal()

```

We can see from these random samples of intercepts and slopes that the lowest cost is `r subset(df_gd, cost == min(cost))$cost` with an intercept of `r subset(df_gd, cost == min(cost))$int` and slope of `r subset(df_gd, cost == min(cost))$slope`. So no matter where we start (any intercept and slope combination), we should descend and ultimately end up with the lowest cost value of `r subset(df_gd, cost == min(cost))$cost`.

<br>

## Partial Derivatives

What are partial derivatives?

Partial derivatives allow us to obtain a slope at any point that is specific to a variable on any function (or line). In our case, partial derivatives allow us to obtain a slope at any specific $\beta$ on the cost function of MSE, which can be denoted as $\frac{\partial MSE}{\partial \hat{\beta}}$. 

<br>

How do we calculate the partial derivatives?

First, we expand MSE.

$$ 
\tag{5}
\begin{align}
MSE & = \frac{(Y-X\beta)^2}{N} \\
& = \frac{(Y-X\beta)(Y-X\beta)}{N} \\
& = \frac{(Y^2-2YX\beta+X^2\beta^2)}{N} \\
& = \frac{Y^2}{N} - \frac{2YX\beta}{N}+\frac{X^2\beta^2}{N} \\
\end{align}
$$

To calculate the partial derivative, we repeat the following for each term:

1. Set the term without $\beta$ to 0 
2. Multiply the term by the exponent of $\beta$
3. Subtract 1 from the exponent of $\beta$

and simplify.

$$
\tag{6}
\begin{align}
\frac{\partial MSE}{\partial \hat{\beta}} & = 0 - \frac{1*2YX\beta^0}{N} + \frac{2X^2\beta^1}{N} \\
& = \frac{-2YX+2X^2\beta}{N} \\
& = \frac{-2X(Y-X\beta)}{N} \\
& = \frac{-2X\epsilon}{N}
\end{align}
$$
Note: We set the term without $\beta$ to 0 in step 1 because that term can be thought of as having $\beta^0$, which equals 1. When we then multiply the term by the exponent of 0, we end up with a term of 0.

<br>

## Update Coefficients

We can then use the information from partial derivatives to update the coefficients ($\beta$) so that coefficients ($\beta$) change and descend into a lower MSE. 

<br>

How do we update the coefficients?

We can update the coefficients $\beta$ by subtracting the partial derivatives multiplied by the learning rate.

$$
\tag{7}
\hat{\beta} = \hat{\beta} - \frac{\partial MSE}{\partial \hat{\beta}}*learning\ rate
$$

A $\beta$ lower than it should be will have a negative slope and when we subtract the partial derivative ($\frac{\partial MSE}{\partial \hat{\beta}}$) from the old $\beta$, the new $\beta$ will then be less negative. Conversely, a $\beta$ higher than should be will have a positive slope and when we subtract the partial derivative ($\frac{\partial MSE}{\partial \hat{\beta}}$), the new $\beta$ will be less positive. When we are at the lowest cost value, the partial derivative ($\frac{\partial MSE}{\partial \hat{\beta}}$) will be 0 and $\beta$ will stop updating.

<br>

What is the learning rate?

The learning rate determines how fast the coefficients update (descends). A higher learning rate descends quickly but may be susceptible to skipping or moving past the global minima. A lower learning rate is more precise but is slower as it takes more time to compute.

```{r}
update_b <- function(y, x, b, lr) {
  y_pred <- x %*% b
  e <- y - y_pred
  derivatives <- (-2 * t(x) %*% e ) / nrow(x)
  b <- b - derivatives * lr
  return(b)
}
```

<br>

## Train

Now let's train the data, arbitrarily starting the coefficients at 0 and a learning rate of 0.0001 for 50,000 iterations.

```{r}
# set number of iterations
iter <- 50000

# set learning rate
lr <- 0.0001

# set initial values of coefficients
b <- NULL
for (i in 1:ncol(x)) {
  b[i] <- 0
}
b <- as.matrix(b)

cat(paste("iteration", "intercept", "slope", "cost\n", 
          "0", round(b[1], n_round), round(b[2], n_round), round(obtain_cost(y, x, b), n_round)))

# set initial training dataset
df_train <- tibble(
  iter = NA,
  int = NA,
  slope = NA,
  cost = NA,
  .rows = iter
)

# train and save training history using for loop
for (i in 1:iter) {
  b <- update_b(y, x, b, lr)
  df_train$iter[i] <- i
  df_train$int[i] <- b[1]
  df_train$slope[i] <- b[2]
  df_train$cost[i] <- obtain_cost(y, x, b)
  if (i %in% round(seq(1, iter, length.out = 10), 0)) {
    cat(paste("\n", round(i, n_round), round(b[1], n_round), round(b[2], n_round), round(obtain_cost(y, x, b), n_round)))
  }
}
```

<br>

### Plot Training

```{r}
ggplot(df_train, aes(iter, cost)) +
  geom_line() +
  theme_minimal() +
  labs(
    x = "iterations",
    y = "cost (mse)"
  )
```

We can see that cost (MSE) flattens out around 15,000 iterations. 

<br>

### Plot Gradient Descent Path

Let's visualize the path that the gradient descent took, starting from our initial intercept and slope value of 0.

```{r}
n_sample <- 100000
df_gd <- tibble(
  int = sample(seq(0, 150000), n_sample, T),
  slope = sample(seq(0, 2000), n_sample, T)
) %>%
  rowwise() %>%
  mutate(cost = obtain_cost(y, x, b = c(int, slope)))

df_train_sorted <- df_train %>% arrange(cost, iter)
df_min <- df_train_sorted[1, ]

ggplot(df_gd, aes(slope, int, color = cost)) +
  geom_point() +
  scale_color_distiller(palette = "Blues") +
  theme_minimal() +
  labs(
    y = "intercept",
    color = "cost (mse)"
  ) +
  geom_line(data = df_train, mapping = aes(slope, int), color = "black", size = 0.5, alpha = 0.5) +
  geom_point(data = df_min, mapping = aes(slope, int), color = "black", shape = 21, alpha = .5) +
  geom_text_repel(
    data = df_min,
    mapping = aes(label = paste0("min(cost) = ", round(cost, n_round), "\nintercept = ", round(int, n_round), "\nslope = ", round(slope, n_round))),
    nudge_y = 30000,
    nudge_x = 1000,
    box.padding = 1.5,
    point.padding = 0.5,
    segment.size = 0.2,
    segment.color = "black",
    color = "black"
  )
```

We can see that most of the iterations were spent optimizing the slope with minimal changes to the intercept as well as the cost (MSE) up until around iteration 1000. Then, the optimal intercept was found within a few iterations, which dramatically reduced cost (MSE).

<br>

## Compare Gradient Descent and `lm()`

We can compare the results of gradient descent with `lm()` to ensure that we did this correctly. We should approximately have the same results within rounding error because OLS is a convex shape with only one global minima.

<br>

### Solve OLS using Gradient Descent
```{r}
coef <- c("(Intercept)" = df_min$int, "yrs.since.phd" = df_min$slope)
rmse_coef <- (solve(t(x) %*% x) * df_min$cost) %>%
  diag() %>%
  sqrt()
t_stat <- coef / rmse_coef
n <- nrow(x)
p <- ncol(x)
df <- n - p
p_value <- 2 * pt(t_stat, df, lower = FALSE)
tibble(
  term = colnames(x),
  estimate = coef,
  std.error = rmse_coef,
  statistic = t_stat,
  p.value = p_value
)
```

<br>

### Solve OLS using `lm()`
```{r}
model <- lm(y ~ x)
tidy(model)
```

<br>

### Cost
```{r}
glue("Cost using GD: {round(df_min$cost, n_round)}\nCost using lm(): {round(glance(model)$sigma^2, n_round)}")
```

We can see that solving OLS regression using gradient descent produced a lower cost function (MSE) than by using `lm()`. The lower MSE using gradient descent was produced from using a smaller intercept. Since MSE was smaller using gradient descent, the standard error was also smaller using gradient descent and subsequently a larger statistic and lower p-value.

<br>

### Figure
```{r}
fig_gd <- ggplot(mapping = aes(x[, 2], y)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = coef[1], slope = coef[2], size = 1) +
  theme_minimal() +
  labs(
    title = "Solved using Gradient Descent",
    x = "yrs.since.phd",
    y = "salary"
  )

fig_lm <- ggplot(mapping = aes(x[, 2], y)) +
  geom_point(alpha = 0.5) +
  geom_smooth(color = "black", method = "lm", se = F) +
  theme_minimal() +
  labs(
    title = "Solved using lm()",
    x = "yrs.since.phd",
    y = "salary"
  )

gridExtra::grid.arrange(fig_gd, fig_lm, nrow = 1)
```

However, since the range of salary is so large, each method produced nearly identical graphs.

<br>

## Acknowledgements

<a href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html" target="_blank">@bfortuner</a> for his gradient descent in python tutorial.

<a href='https://github.com/mychan24' target='_blank'>@mychan24</a> for her helpful feedback and suggestions to this blog.

<a href='https://github.com/praxling' target='_blank'>@praxling</a> for her proofing and feedback to this blog.

<br>

<hr>

<br>

<!-- disqus START -->
<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = 'https://ekarinpongpipat.com/blog_solving_ols_regression_using_gradient_descent.html';  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = 'blog_solving_ols_regression_using_gradient_descent'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://epongpipat.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<!-- disqus END --></div>
</div>
</div>
</div>
<div class="footer" style="red">
  <hr style="solid 1px">
  &copy; 2019 Ekarin Eric Pongpipat. <a href="https://raw.githubusercontent.com/epongpipat/epongpipat.github.io/master/LICENSE" target="_blank">All rights reserved.</a>
</div>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeSourceEmbed("blog-solving-ols-regression-using-gradient-descent.Rmd");
  window.initializeCodeFolding("show" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
